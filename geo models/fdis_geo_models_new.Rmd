---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---


```{r}
library(dplyr)
library(tidyr) # For drop_na and pivot_longer
library(ggplot2)
library(INLA)
library(purrr) # For map functions
library(tibble)
library(gt)    # For creating nice tables
library(stringr) # For string manipulation (if needed for grid_id)
library(sf)      # For handling spatial objects
library(spdep)   # For defining neighbors and exporting graph
library(readr)
```

```{r}
set.seed(123)
```


```{r}
# --- load ----------
dat <- read_csv("output/tei/combined_tei_and_environmental_data.csv", na = c("", "NA"))
ses_mpd <- read_csv("output/traits_geo_model/spatial_ses_mpd.csv", na = c("", "NA"))
fd_raw <- read_csv(
  "output/acoustic_diversity/ses_fd_maps_random_assembly/ses_fdis_random_assembly_results_full.csv",
  na = c("", "NA")
)
```
## Data preparation
```{r}
# --- Corrected and Robust Merge for ses_mpd ---

# 1. Ensure ses_mpd is clean
ses_mpd_clean <- ses_mpd %>%
  dplyr::filter(!is.na(grid_id)) 

# 2. Run Regex on the clean data
id_col <- "grid_id"
coord_pattern <- "^lon(n?)(\\d+)p.*_lat(n?)(\\d+)p.*$"
extracted_coords <- stringr::str_match(ses_mpd_clean[[id_col]], coord_pattern)

# 3. Create the coordinates dataframe directly
# We use cbind to safely attach the extracted matrix to the dataframe
ses_mpd_coords <- ses_mpd_clean %>%
  dplyr::mutate(
    lon_sign_char = extracted_coords[, 2],
    lon_val_char  = extracted_coords[, 3],
    lat_sign_char = extracted_coords[, 4],
    lat_val_char  = extracted_coords[, 5]
  ) %>%
  dplyr::mutate(
    # Logic: if "n" then -1, else 1. Handle NAs if regex failed for a row.
    lon_sign = ifelse(!is.na(lon_sign_char) & lon_sign_char == "n", -1, 1),
    lon      = lon_sign * as.numeric(lon_val_char),
    
    lat_sign = ifelse(!is.na(lat_sign_char) & lat_sign_char == "n", -1, 1),
    lat      = lat_sign * as.numeric(lat_val_char),
    
    ses_mpd  = as.numeric(ses_mpd)
  ) %>%
  # Filter out any rows where coordinates failed to parse (became NA)
  dplyr::filter(!is.na(lon) & !is.na(lat)) %>%
  dplyr::select(lon, lat, ses_mpd) %>%
  # Handle duplicate cells if any
  dplyr::group_by(lon, lat) %>%
  dplyr::summarise(ses_mpd = mean(ses_mpd, na.rm = TRUE), .groups = "drop")

# 4. Check before joining
cat("Rows in ses_mpd_coords:", nrow(ses_mpd_coords), "\n")
cat("Columns in ses_mpd_coords:", paste(colnames(ses_mpd_coords), collapse=", "), "\n")

if(!all(c("lon", "lat") %in% colnames(ses_mpd_coords))){
  stop("CRITICAL: ses_mpd_coords failed to create lon/lat columns.")
}

# 5. Join safely
dat <- dat %>%
  dplyr::left_join(ses_mpd_coords, by = c("lon", "lat"))

cat("Success! ses_mpd merged. NA count:", sum(is.na(dat$ses_mpd)), "\n")
```


```{r}
# --- Corrected and Robust Merge for ses_fdis ---

# 1. Clean NAs first to ensure alignment
fd_clean <- fd_raw %>%
  dplyr::filter(!is.na(grid_id)) 

# 2. Run Regex on the CLEAN data specifically
fd_id_col <- "grid_id"
fd_coord_pattern <- "^lon(n?)(\\d+)p.*_lat(n?)(\\d+)p.*$"
fd_extracted <- stringr::str_match(fd_clean[[fd_id_col]], fd_coord_pattern)

# 3. Create the coordinates dataframe safely
ses_fdis_coords <- fd_clean %>%
  dplyr::mutate(
    # Extract components into temp columns first
    lon_sign_char = fd_extracted[, 2],
    lon_val_char  = fd_extracted[, 3],
    lat_sign_char = fd_extracted[, 4],
    lat_val_char  = fd_extracted[, 5]
  ) %>%
  dplyr::mutate(
    # Calculate numeric coordinates
    lon_sign = ifelse(!is.na(lon_sign_char) & lon_sign_char == "n", -1, 1),
    lon      = lon_sign * as.numeric(lon_val_char),
    
    lat_sign = ifelse(!is.na(lat_sign_char) & lat_sign_char == "n", -1, 1),
    lat      = lat_sign * as.numeric(lat_val_char),
    
    ses_fdis = as.numeric(ses_fdis)
  ) %>%
  # Filter out any rows where coordinates failed to parse
  dplyr::filter(!is.na(lon) & !is.na(lat)) %>%
  dplyr::select(lon, lat, ses_fdis) %>%
  # Handle duplicate cells (aggregating if necessary)
  dplyr::group_by(lon, lat) %>%
  dplyr::summarise(ses_fdis = mean(ses_fdis, na.rm = TRUE), .groups = "drop")

# 4. Validation check before joining
cat("Rows in ses_fdis_coords:", nrow(ses_fdis_coords), "\n")
if(!all(c("lon", "lat", "ses_fdis") %in% colnames(ses_fdis_coords))){
  stop("CRITICAL: ses_fdis_coords failed to create required columns.")
}

# 5. Join safely
dat <- dat %>%
  dplyr::left_join(ses_fdis_coords, by = c("lon", "lat"))

cat("Success! ses_fdis merged. Available for", sum(!is.na(dat$ses_fdis)), "rows.\n")
```


```{r}
# --- Define Columns ---
fdis_col <- "ses_fdis"

# Use separate vegetation predictors (no composite index)
predictor_cols <- c("rh_mean", "temp_mean", "ruggedness", "hfp",
                    "ndvi_mean", "vegetation_height", "ses_mpd")

# Raw columns needed
veg_raw_cols <- c("ndvi_mean", "veg_height_avg")
other_raw_predictors <- c("rh_mean", "temp_mean", "ruggedness", "hfp")
all_raw_predictors_initially <- c(other_raw_predictors, veg_raw_cols, "ses_mpd", fdis_col)

```


```{r}
# --- Data Cleaning and Final Selection (no vegetation_index) ---

stopifnot(all(c("lon","lat") %in% names(dat)))

dat_processed <- dat %>%
  dplyr::select(lon, lat, dplyr::all_of(all_raw_predictors_initially)) %>%
  dplyr::mutate(dplyr::across(dplyr::all_of(c(other_raw_predictors, veg_raw_cols, "ses_mpd", fdis_col)),
                              as.numeric)) %>%
  # Create requested "vegetation_height" from raw veg_height_avg
  dplyr::mutate(vegetation_height = veg_height_avg) %>%
  dplyr::select(lon, lat, dplyr::all_of(fdis_col),
                rh_mean, temp_mean, ruggedness, hfp,
                ndvi_mean, vegetation_height, ses_mpd)

dat <- dat_processed %>%
  tidyr::drop_na() %>%
  dplyr::arrange(lon, lat) %>%
  dplyr::mutate(idx = dplyr::row_number())

print(paste("Remaining rows:", nrow(dat)))
print("Summary of vegetation predictors:")
print(summary(dat[, c("ndvi_mean", "vegetation_height")]))
```

```{r}
# --- Visualize FDIS Distribution ---
# Calculate statistics
fdis_mean <- mean(dat[[fdis_col]], na.rm = TRUE)
fdis_sd <- sd(dat[[fdis_col]], na.rm = TRUE)

plot_fdis <- ggplot(dat, aes(x = .data[[fdis_col]])) +
  geom_histogram(bins = 30, fill = "steelblue", alpha = 0.8) +
  # Add vertical line for mean
  geom_vline(xintercept = fdis_mean, color = "red", linewidth = 1, linetype = "dashed") +
  # Add shaded region for ±1 SD
  annotate("rect", 
           xmin = fdis_mean - fdis_sd, 
           xmax = fdis_mean + fdis_sd,
           ymin = 0, ymax = Inf,
           alpha = 0.2, fill = "red") +
  # Add text annotation
  annotate("text", 
           x = fdis_mean, 
           y = Inf, 
           label = sprintf("Mean = %.3f\nSD = %.3f", fdis_mean, fdis_sd),
           vjust = 1.5, 
           hjust = 0.5,
           size = 3.5,
           color = "red") +
  labs(title = "Distribution of FDIS", 
       x = "FDIS", 
       y = "Frequency") +
  theme_minimal()

print(plot_fdis)

# Optionally print the statistics
cat(sprintf("\nFDIS Statistics:\n  Mean: %.4f\n  SD: %.4f\n  n: %d\n", 
            fdis_mean, fdis_sd, sum(!is.na(dat[[fdis_col]]))))
```

```{r}
# --- Step 1a: Find Islands in the ORIGINAL data ---
cat("--- Running initial neighbor search to identify islands --- \n")

dat_original <- dat # Create a copy if 'dat' h

# Verify required columns exist in dat_original
if (!all(c("lon", "lat", "idx") %in% names(dat_original))) {
  stop("Original data frame must contain 'lon', 'lat', and 'idx' columns.")
}
if (!all(dat_original$idx == 1:nrow(dat_original))) {
   warning("Re-creating sequential 'idx' in original data copy.")
   dat_original <- dat_original %>% arrange(idx) %>% mutate(idx = 1:n())
}


# --- Run the neighbor finding loop (same as before) ---
coord_map_orig <- setNames(as.list(dat_original$idx), paste(round(dat_original$lon, 5), round(dat_original$lat, 5), sep = "_"))
neighbours_list_orig <- vector("list", nrow(dat_original))
max_lon_orig <- 179
min_lon_orig <- -180

cat("Finding neighbors on original data (to identify islands)...\n")
for (i in 1:nrow(dat_original)) {
  current_idx <- dat_original$idx[i]
  current_lon <- dat_original$lon[i]
  current_lat <- dat_original$lat[i]
  neighbor_indices <- integer(0)
  relative_coords <- list(c(0, 1), c(0, -1), c(1, 0), c(-1, 0), c(1, 1), c(1, -1), c(-1, 1), c(-1, -1))
  for (dcoords in relative_coords) {
    dlon <- dcoords[1]; dlat <- dcoords[2]
    target_lon <- current_lon + dlon; target_lat <- current_lat + dlat
    if (target_lon > max_lon_orig) { target_lon <- min_lon_orig }
    else if (target_lon < min_lon_orig) { target_lon <- max_lon_orig }
    target_key <- paste(round(target_lon, 5), round(target_lat, 5), sep = "_")
    if (target_key %in% names(coord_map_orig)) {
      neighbor_indices <- c(neighbor_indices, coord_map_orig[[target_key]])
    }
  }
  # Store based on the original index 'i' which matches current_idx here
  neighbours_list_orig[[i]] <- sort(unique(as.integer(neighbor_indices)))
   if (i %% 5000 == 0) { cat("Processed", i, "of", nrow(dat_original), "cells...\n") }
}
cat("Finished initial neighbor search.\n")

# --- Identify island indices (based on the original index) ---
island_indices_orig <- which(sapply(neighbours_list_orig, length) == 0)

if (length(island_indices_orig) > 0) {
  cat("Identified", length(island_indices_orig), "island cells (based on original indices):\n")
  print(head(island_indices_orig))
} else {
  cat("No island cells identified.\n")
}
cat("--- End of island identification --- \n\n")


# --- Step 1b: Filter the main dataframe 'dat' to REMOVE islands ---
if (length(island_indices_orig) > 0) {
    cat("Removing", length(island_indices_orig), "island cells from the main 'dat' dataframe.\n")
    # Filter rows where the original index is NOT in the island list
    dat <- dat_original %>%
        filter(! (idx %in% island_indices_orig) ) %>%
        # CRUCIAL: Re-create the sequential index for the filtered data
        mutate(idx = 1:n())

    cat("Dataframe 'dat' now has", nrow(dat), "rows after removing islands.\n")
    cat("New range of 'idx':", paste(range(dat$idx), collapse=" to "), "\n")
} else {
    cat("No islands to remove. Proceeding with original data.\n")
    # Ensure idx is sequential just in case
    dat <- dat_original %>% mutate(idx = 1:n())
}
```


```{r}
# --- Create Spatial Neighborhood Graph File (Grid Indexing Method) ---

# --- Step 1: Ensure Data and Coordinates are Ready ---

# Verify required columns exist
if (!all(c("lon", "lat", "idx") %in% names(dat))) {
  stop("Data frame 'dat' must contain 'lon', 'lat', and 'idx' columns.")
}
# Ensure idx is sequential (this check might have been done earlier, but good practice)
if (!all(dat$idx == 1:nrow(dat))) {
   warning("Re-creating sequential 'idx' column to match row order.")
   dat <- dat %>% arrange(idx) %>% mutate(idx = 1:n())
}

# --- Step 2: Find Neighbors using Coordinate Logic (Queen Contiguity) ---
cat("Finding neighbors based on grid coordinates (Queen Contiguity)...\n")
coord_map <- setNames(as.list(dat$idx), paste(round(dat$lon, 5), round(dat$lat, 5), sep = "_"))
neighbours_list_manual <- vector("list", nrow(dat))
max_lon <- 179
min_lon <- -180

for (i in 1:nrow(dat)) {
  current_idx <- dat$idx[i]
  current_lon <- dat$lon[i]
  current_lat <- dat$lat[i]
  neighbor_indices <- integer(0)
  relative_coords <- list(
    c(0, 1), c(0, -1), c(1, 0), c(-1, 0),
    c(1, 1), c(1, -1), c(-1, 1), c(-1, -1)
  )
  for (dcoords in relative_coords) {
    dlon <- dcoords[1]; dlat <- dcoords[2]
    target_lon <- current_lon + dlon; target_lat <- current_lat + dlat
    if (target_lon > max_lon) { target_lon <- min_lon }
    else if (target_lon < min_lon) { target_lon <- max_lon }
    target_key <- paste(round(target_lon, 5), round(target_lat, 5), sep = "_")
    if (target_key %in% names(coord_map)) {
      neighbor_indices <- c(neighbor_indices, coord_map[[target_key]])
    }
  }
  neighbours_list_manual[[current_idx]] <- sort(unique(as.integer(neighbor_indices)))
   if (i %% 1000 == 0) { cat("Processed", i, "of", nrow(dat), "cells...\n") }
}
cat("Finished finding neighbors.\n") # Added confirmation message

# --- Identify islands (cells with 0 neighbors) ---
cat("Checking for cells with zero neighbours (islands)...\n")
island_indices <- which(sapply(neighbours_list_manual, length) == 0)

if (length(island_indices) > 0) {
  cat("Found", length(island_indices), "island(s) with index (row number in 'dat'):\n")
  # Show first few island indices
  print(head(island_indices))
  cat("These cells have no neighbors within the dataset.\n")
  # Optional: Show coordinates of first few islands
  cat("Coordinates of first few potential islands:\n")
  print(head(dat[island_indices, c("idx", "lon", "lat")]))
} else {
  cat("No cells with zero neighbours found.\n")
}


# --- Step 3: Convert the List to an 'nb' Object ---
cat("Converting list to nb object...\n")
# Set attributes required for an nb object
class(neighbours_list_manual) <- "nb"
attr(neighbours_list_manual, "region.id") <- as.character(dat$idx) # Use idx as region IDs
attr(neighbours_list_manual, "queen") <- TRUE # Indicate it's Queen contiguity
coords.mat <- as.matrix(dat[, c("lon", "lat")])
attr(neighbours_list_manual, "coords") <- coords.mat


# --- Step 4: Export Neighbors to INLA Graph Format ---
graph_file_path <- "output/model_data/grid_neighbours_corrected.graph" # Use a new file name
cat("Writing corrected INLA graph file to:", graph_file_path, "\n")

# Attempt to write the graph file using nb2INLA
# nb2INLA should be more robust to islands than summary/validation functions
tryCatch({
    nb2INLA(graph_file_path, neighbours_list_manual)
    cat("Corrected graph file successfully created.\n")
}, error = function(e) {
    cat("ERROR: Failed to write graph file using nb2INLA:", conditionMessage(e), "\n")
    cat("This might indicate a more fundamental issue with the nb object structure.\n")
})
```

```{r}
# --- Coordinate preparation & projection (single source of truth) ---
library(sf)

stopifnot(all(c("lon","lat") %in% names(dat)))

# Build sf and project to a global equal-area CRS (Mollweide here)
sf_pts_ll  <- sf::st_as_sf(dat, coords = c("lon","lat"), crs = 4326)
crs_target <- "+proj=moll +lon_0=0 +datum=WGS84 +units=m +no_defs"
sf_pts     <- sf::st_transform(sf_pts_ll, crs_target)

# Projected numeric coordinates for all distance-based ops (Moran's I, mesh, etc.)
coords_xy <- sf::st_coordinates(sf_pts)
colnames(coords_xy) <- c("x","y")

dat$lon <- sf::st_coordinates(sf_pts_ll)[,1]
dat$lat <- sf::st_coordinates(sf_pts_ll)[,2]
```


```{r}
# --- Scale Predictor Variables (define veg_pc1; drop NDVI & veg height from predictors) ---

# Ensure vegetation PC1 exists; if not, compute it from NDVI & vegetation height, then drop raw veg vars from predictors.

if (!"veg_pc1" %in% names(dat)) {
stopifnot(all(c("ndvi_mean","vegetation_height") %in% names(dat)))
Xveg <- scale(dat[, c("ndvi_mean","vegetation_height")], center = TRUE, scale = TRUE)
pca_veg <- prcomp(as.matrix(Xveg), center = FALSE, scale. = FALSE)
dat$veg_pc1 <- as.numeric(scale(pca_veg$x[, 1])[, 1])
s_orient <- sign(
suppressWarnings(cor(dat$veg_pc1, Xveg[,1], use = "complete.obs")) +
suppressWarnings(cor(dat$veg_pc1, Xveg[,2], use = "complete.obs"))
)
if (!is.na(s_orient) && s_orient < 0) dat$veg_pc1 <- -dat$veg_pc1
}

# Final predictor set (no NDVI or vegetation height)

predictor_cols <- c("rh_mean","temp_mean","ruggedness","hfp","veg_pc1","ses_mpd")

# Scale predictors

dat <- dat %>%
dplyr::mutate(dplyr::across(dplyr::all_of(predictor_cols), ~ scale(.)[,1]))

cat("Scaled predictors summary (final set):\n")
print(summary(dat[, predictor_cols]))

```

```{r}
# --- Visualize Predictor Maps (After Scaling) ---

stopifnot(all(c("lon","lat","idx", predictor_cols) %in% names(dat)))

cat("\n--- Creating maps for each scaled predictor variable ---\n")

# Build 1x1° polygons

polygon_list_predictors <- lapply(1:nrow(dat), function(i) {
lon_bl <- dat$lon[i]; lat_bl <- dat$lat[i]
corners <- matrix(c(
lon_bl,     lat_bl,
lon_bl + 1, lat_bl,
lon_bl + 1, lat_bl + 1,
lon_bl,     lat_bl + 1,
lon_bl,     lat_bl
), ncol = 2, byrow = TRUE)
sf::st_polygon(list(corners))
})
sfc_polygons_predictors <- sf::st_sfc(polygon_list_predictors, crs = 4326)
dat_sf_predictors <- sf::st_sf(dat[, predictor_cols, drop = FALSE], geometry = sfc_polygons_predictors)

# Plot per predictor

for (pred_col in predictor_cols) {
cat("Map:", pred_col, "\n")
p <- ggplot2::ggplot() +
ggplot2::geom_sf(data = dat_sf_predictors, ggplot2::aes_string(fill = pred_col), color = NA) +
ggplot2::scale_fill_viridis_c(option = "viridis", na.value="grey80") +
ggplot2::labs(title = paste("Map of Scaled Predictor:", pred_col), fill = "z-score") +
ggplot2::theme_minimal() +
ggplot2::theme(axis.text = element_blank(), axis.ticks = element_blank(), panel.grid = element_blank())
print(p)
}
cat("--- Finished generating predictor maps ---\n")
```

```{r}
# --- Map of FDIS ---

stopifnot(all(c("lon","lat","idx", fdis_col) %in% names(dat)))

poly_list_fdis <- lapply(1:nrow(dat), function(i){
lon_bl <- dat$lon[i]; lat_bl <- dat$lat[i]
sf::st_polygon(list(matrix(c(lon_bl,lat_bl, lon_bl+1,lat_bl, lon_bl+1,lat_bl+1, lon_bl,lat_bl+1, lon_bl,lat_bl),
ncol=2, byrow=TRUE)))
})
dat_sf_fdis <- sf::st_sf(dat[fdis_col], geometry = sf::st_sfc(poly_list_fdis, crs = 4326))

p_fdis_map <- ggplot2::ggplot() +
ggplot2::geom_sf(data = dat_sf_fdis, ggplot2::aes(fill = .data[[fdis_col]]), color = NA) +
ggplot2::scale_fill_viridis_c(option = "cividis", na.value="grey80") +
ggplot2::labs(title = "Map of FDIS", fill = "ses_fdis") +
ggplot2::theme_minimal() +
ggplot2::theme(axis.text = element_blank(), axis.ticks = element_blank(), panel.grid = element_blank())
print(p_fdis_map)
```

```{r}
# --- Cell-wise correlation contribution maps (no neighbourhood averaging) -----
# Idea: r = mean(zx * zy). Map each cell's contribution c_i = z_xi * z_yi.
# This highlights large-scale biogeographic structure without local smoothing.
library(sf); library(ggplot2); library(scales)
stopifnot(all(c(fdis_col, predictor_cols, "lon", "lat") %in% names(dat)))

# z-scores once
z_fdis <- as.numeric(scale(dat[[fdis_col]]))

# compute per-variable cell-wise contributions and store for global limits
contrib_list <- list()
for (pred in predictor_cols) {
  z_pred <- as.numeric(scale(dat[[pred]]))
  contrib_list[[pred]] <- z_fdis * z_pred
}

# global symmetric limits (robust to outliers)
abs_all <- abs(unlist(contrib_list))
lim <- suppressWarnings(as.numeric(stats::quantile(abs_all, 0.99, na.rm = TRUE)))
if (!is.finite(lim) || lim == 0) lim <- max(abs_all, na.rm = TRUE)

# build 1×1° polygons once
polys <- lapply(seq_len(nrow(dat)), function(i) {
  x <- dat$lon[i]; y <- dat$lat[i]
  st_polygon(list(matrix(c(x,y, x+1,y, x+1,y+1, x,y+1, x,y), ncol=2, byrow=TRUE)))
})
geom_sfc <- st_sfc(polys, crs = 4326)

# plot each map using the same limits
for (pred in predictor_cols) {
  cname <- paste0("corr_cell_", pred)
  dat[[cname]] <- contrib_list[[pred]]
  
  # Get correlation and p-value
  cor_test <- suppressWarnings(cor.test(dat[[fdis_col]], dat[[pred]]))
  r_global <- cor_test$estimate
  p_value <- cor_test$p.value
  
  # Add significance stars
  sig <- ifelse(p_value < 0.001, "***", 
                ifelse(p_value < 0.01, "**", 
                       ifelse(p_value < 0.05, "*", "ns")))
  
  cat(sprintf("global r(ses_fdis, %s) = %.3f, p = %.3e %s\n", 
              pred, r_global, p_value, sig))
  
  sf_map <- st_sf(dat[cname], geometry = geom_sfc)
  
  p <- ggplot(sf_map) +
    geom_sf(aes(fill = .data[[cname]]), color = NA) +
    scale_fill_gradient2(
      name = "z(fdis) × z(var)",
      limits = c(-lim, lim),
      oob = squish,
      na.value = "grey85"
    ) +
    labs(title = paste0("cell-wise correlation contribution: fdis × ", pred),
         subtitle = sprintf("r = %.3f, p = %.3e %s", r_global, p_value, sig)) +
    theme_minimal() +
    theme(
      axis.text = element_blank(),
      axis.ticks = element_blank(),
      panel.grid = element_blank()
    )
  print(p)
}
```

```{r}
# --- Correlations (keep only veg_pc1 among vegetation terms) ---
library(corrplot)
library(gridExtra)
stopifnot(all(c(fdis_col, predictor_cols) %in% names(dat)))
cor_matrix <- cor(dat[, c(fdis_col, predictor_cols)], use = "complete.obs")
cat("\n--- Pearson correlations with FDIS ---\n")
cor_with_fdis <- cor_matrix[fdis_col, predictor_cols]
print(data.frame(Predictor = predictor_cols,
                 r = round(cor_with_fdis[predictor_cols], 3))[
                   order(-abs(cor_with_fdis[predictor_cols])), ])

# Corrplot (includes veg_pc1 and other predictors; no raw NDVI/height)
corrplot(cor_matrix, method = "color", type = "upper",
         order = "hclust", tl.col = "black", tl.srt = 45,
         addCoef.col = "black", number.cex = 0.7,
         main = "Correlation Matrix: FDIS and Predictors (incl. veg_pc1)")

# Bivariate scatter + smoothers
plot_list <- lapply(predictor_cols, function(pred) {
  ggplot2::ggplot(dat, ggplot2::aes_string(x = pred, y = fdis_col)) +
    ggplot2::geom_point(alpha = 0.1, size = 0.5) +
    ggplot2::geom_smooth(method = "loess", color = "red", se = TRUE) +
    ggplot2::geom_smooth(method = "lm", color = "blue", se = FALSE, linetype = "dashed") +
    ggplot2::labs(title = paste("FDIS vs", pred),
                  subtitle = paste("r =", round(cor_with_fdis[pred], 3))) +
    ggplot2::theme_minimal() +
    ggplot2::theme(plot.title = element_text(size = 10),
                   plot.subtitle = element_text(size = 8))
})
gridExtra::grid.arrange(grobs = plot_list, ncol = 3)

# Bivariate with 2D density distribution + smooth curves
plot_list_density <- lapply(predictor_cols, function(pred) {
  ggplot2::ggplot(dat, ggplot2::aes_string(x = pred, y = fdis_col)) +
    ggplot2::stat_density_2d(aes(fill = ..level..), geom = "polygon", alpha = 0.5) +
    ggplot2::scale_fill_gradient(low = "lightblue", high = "darkblue") +
    ggplot2::geom_smooth(method = "loess", color = "red", se = TRUE, linewidth = 1) +
    ggplot2::geom_smooth(method = "lm", color = "blue", se = FALSE, 
                         linetype = "dashed", linewidth = 1) +
    ggplot2::labs(title = paste("FDIS vs", pred, "(Density)"),
                  subtitle = paste("r =", round(cor_with_fdis[pred], 3))) +
    ggplot2::theme_minimal() +
    ggplot2::theme(plot.title = element_text(size = 10),
                   plot.subtitle = element_text(size = 8),
                   legend.position = "none")
})

gridExtra::grid.arrange(grobs = plot_list_density, ncol = 3)
```

# Models' types comparisons 

```{r}
# --- OLS, BYM2, SPDE (using veg_pc1) + spatial autocorrelation diagnostics ---
library(spdep)
library(INLA)

# OLS
formula_ols <- as.formula(paste(fdis_col, "~", paste(predictor_cols, collapse = " + ")))
model_ols <- lm(formula_ols, data = dat)
resid_ols <- residuals(model_ols)

# Spatial weights for Moran's I
# kNN on projected meters; keep k = 8 (tune if needed)
nb_obj <- spdep::knn2nb(spdep::knearneigh(coords_xy, k = 8))
listw_obj <- spdep::nb2listw(nb_obj, style = "W", zero.policy = TRUE)

# Now compute Moran's I using these weights
moran_ols <- spdep::moran.test(residuals(model_ols), listw_obj, zero.policy = TRUE)

# BYM2 (graph file built previously)
stopifnot(exists("graph_file_path"))
g <- INLA::inla.read.graph(graph_file_path)

form_bym2 <- as.formula(paste(
  "ses_fdis ~",
  paste(predictor_cols, collapse = " + "),
  "+ f(idx, model = 'bym2', graph = g, scale.model = TRUE,",
  "hyper = list(prec = list(prior = 'pc.prec', param = c(0.5, 0.01)),",
  "phi = list(prior = 'pc', param = c(0.5, 0.5))))"
))

model_bym2 <- INLA::inla(
  form_bym2,
  family = "gaussian",
  data = dat,
  control.compute = list(dic = TRUE, waic = TRUE),
  control.predictor = list(compute = TRUE),
  verbose = FALSE
)

fitted_bym2 <- model_bym2$summary.fitted.values$mean[1:nrow(dat)]
resid_bym2 <- dat$ses_fdis - fitted_bym2
moran_bym2 <- spdep::moran.test(resid_bym2, listw_obj, zero.policy = TRUE)

# --- SPDE in a projected CRS (planar mesh; consistent with diagnostics) ---
# Mesh over projected coords (meters)
mesh <- INLA::inla.mesh.2d(
  loc = coords_xy,
  max.edge = c(2e5, 5e5),  # ~200 km inner, ~500 km outer; tune to data density/scale
  cutoff = 5e4,            # ~50 km minimum point separation; tune
  offset = c(2e5, 8e5)     # domain extension
)

# PC-Matérn priors in meters; example: P(range < 1000 km) = 0.01; P(sigma > 0.5) = 0.01
spde <- INLA::inla.spde2.pcmatern(
  mesh = mesh,
  prior.range = c(1e6, 0.01),  # range0 = 1,000 km
  prior.sigma = c(0.5, 0.01)
)

# Projected A matrix using projected coords
A <- INLA::inla.spde.make.A(mesh = mesh, loc = coords_xy)

# Build stack with fixed effects
stack_spde <- INLA::inla.stack(
  data = list(y = dat[[fdis_col]]),
  A = list(A, 1),
  effects = list(
    s = 1:spde$n.spde,
    cbind(Intercept = 1, dat[, predictor_cols, drop = FALSE])
  ),
  tag = "est"
)

form_spde <- as.formula(
  paste("y ~ -1 + Intercept +",
        paste(predictor_cols, collapse = " + "),
        "+ f(s, model = spde)")
)

result_spde <- INLA::inla(
  form_spde,
  family = "gaussian",
  data = INLA::inla.stack.data(stack_spde),
  control.predictor = list(
    A = INLA::inla.stack.A(stack_spde),
    compute = TRUE
  ),
  control.compute = list(dic = TRUE, waic = TRUE, cpo = TRUE, config = TRUE),
  control.inla = list(
    strategy = "adaptive",
    int.strategy = "eb",
    diagonal = 1e-3
  ),
  verbose = FALSE
)

# Use a single, consistent fitted vector (never overwrite elsewhere)
idx_est <- INLA::inla.stack.index(stack_spde, "est")$data
fitted_spde <- result_spde$summary.fitted.values$mean[idx_est]
resid_spde <- dat[[fdis_col]] - fitted_spde

# Moran's I for SPDE residuals on projected neighbors
moran_spde <- spdep::moran.test(resid_spde, listw_obj, zero.policy = TRUE)

# Comparison table
var_y <- var(dat[[fdis_col]], na.rm = TRUE)
r2_ols <- 1 - var(residuals(model_ols), na.rm = TRUE) / var_y
r2_bym2 <- 1 - var(resid_bym2, na.rm = TRUE) / var_y
r2_spde <- 1 - var(resid_spde, na.rm = TRUE) / var_y

spatial_comparison <- data.frame(
  Model = c("OLS", "BYM2", "SPDE"),
  DIC = c(
    NA,
    round(model_bym2$dic$dic, 1),
    round(result_spde$dic$dic, 1)
  ),
  WAIC = c(
    NA,
    round(model_bym2$waic$waic, 1),
    round(result_spde$waic$waic, 1)
  ),
  Moran_I = c(
    round(moran_ols$estimate[1], 3),
    round(moran_bym2$estimate[1], 3),
    round(moran_spde$estimate[1], 3)
  ),
  p_value = c(
    format.pval(moran_ols$p.value),
    format.pval(moran_bym2$p.value),
    format.pval(moran_spde$p.value)
  ),
  R_squared = c(
    round(r2_ols, 3),
    round(r2_bym2, 3),
    round(r2_spde, 3)
  )
)

print(spatial_comparison)
```


```{r}
## --- Robust variance decomposition with full reporting (SPDE) ---
##   result_spde, dat, predictor_cols, tei_col, A, idx_est

# 0) Helpers
fmt <- function(x) formatC(x, digits = 6, format = "f")
pct <- function(x) sprintf("%.2f%%", 100 * x)

# 1) Components at data locations (fixed and spatial, posterior means)
beta <- result_spde$summary.fixed$mean
names(beta) <- rownames(result_spde$summary.fixed)

Xfix <- cbind(Intercept = 1, dat[, predictor_cols, drop = FALSE])
fixed_pred <- as.numeric(as.matrix(Xfix) %*% beta)

spatial_at_data <- as.numeric(A %*% result_spde$summary.random$s$mean)

# 2) One fitted definition for EVERYTHING
fitted_sum <- fixed_pred + spatial_at_data

# 3) Common mask to prevent NA / row-set drift
y <- dat[[fdis_col]]
keep <- is.finite(y) & is.finite(fixed_pred) & is.finite(spatial_at_data)

y_k     <- y[keep]
fixed_k <- fixed_pred[keep]
spat_k  <- spatial_at_data[keep]
fit_k   <- fitted_sum[keep]

# (optional) fitted from INLA stack to diagnose ordering mismatches
sfit_all <- result_spde$summary.fitted.values$mean[idx_est]
sfit_k   <- sfit_all[keep]

# 4) Variances and covariance on the SAME rows
var_y    <- var(y_k)
var_fix  <- var(fixed_k)
var_spat <- var(spat_k)
cov_fs   <- cov(fixed_k, spat_k)
cor_fs   <- cor(fixed_k, spat_k)

# 5) Decomposition
var_fit     <- var(fit_k)
var_parts   <- var_fix + var_spat + 2 * cov_fs
R2_total    <- var_fit / var_y                        # == 1 - var(y_k - fit_k)/var_y
R2_fixed    <- var_fix  / var_y
R2_spatial  <- var_spat / var_y
R2_shared   <- (2 * cov_fs) / var_y
R2_residual <- 1 - R2_total

# 6) Reporting
cat("=== VARIANCE DECOMPOSITION (SPDE) ===\n")
cat(sprintf("N_total = %d | N_used = %d | dropped = %d\n",
            length(y), length(y_k), sum(!keep)))

cat("\n--- Variances and covariance (on used rows) ---\n")
cat(sprintf("var(y)                     = %s\n", fmt(var_y)))
cat(sprintf("var(Fixed)                 = %s\n", fmt(var_fix)))
cat(sprintf("var(Spatial)               = %s\n", fmt(var_spat)))
cat(sprintf("cov(Fixed, Spatial)        = %s\n", fmt(cov_fs)))
cat(sprintf("cor(Fixed, Spatial)        = %.6f\n", cor_fs))
cat(sprintf("var(Fixed + Spatial)       = %s\n", fmt(var_fit)))
cat(sprintf("var(F)+var(S)+2*cov(F,S)   = %s\n", fmt(var_parts)))
cat(sprintf("variance closure diff      = %.3e\n", var_fit - var_parts))

# alignment diagnostic vs INLA's fitted means on the same rows
cat(sprintf("max| (F+S) - fitted_stack | = %.3e\n",
            max(abs(fit_k - sfit_k), na.rm = TRUE)))

cat("\n--- R^2 components (fractions of var(y)) ---\n")
cat(sprintf("R2_total                   = %s  (%s)\n", fmt(R2_total),    pct(R2_total)))
cat(sprintf("R2_fixed                   = %s  (%s)\n", fmt(R2_fixed),    pct(R2_fixed)))
cat(sprintf("R2_spatial                 = %s  (%s)\n", fmt(R2_spatial),  pct(R2_spatial)))
cat(sprintf("R2_shared (2*cov/var(y))   = %s  (%s)\n", fmt(R2_shared),   pct(R2_shared)))
cat(sprintf("R2_residual                = %s  (%s)\n", fmt(R2_residual), pct(R2_residual)))
cat(sprintf("R^2 closure error          = %.3e\n",
            (R2_fixed + R2_spatial + R2_shared + R2_residual) - 1))

# 7) Tidy summary table
report <- data.frame(
  Metric  = c("R2_total","R2_fixed","R2_spatial","R2_shared","R2_residual"),
  Value   = c(R2_total, R2_fixed, R2_spatial, R2_shared, R2_residual),
  Percent = c(pct(R2_total), pct(R2_fixed), pct(R2_spatial), pct(R2_shared), pct(R2_residual))
)
cat("\n=== R^2 SUMMARY TABLE ===\n")
print(report, row.names = FALSE)
```



## Cross validation selection 


```{r}
# ========================= FOUR-CONFIG SELECTION RUNNER (FIXED) ===============
# Configs:
#   C1: no interactions, winsorize |z|<=2,  WITHOUT ses_mpd
#   C2: no interactions, z-score (no clipping), WITHOUT ses_mpd
#   C3: WITH interactions (pairwise lin×lin), winsorize |z|<=2, WITHOUT ses_mpd
#   C4: no interactions, winsorize |z|<=2,  WITH ses_mpd
# Output CSV: "model_selection_runs.csv"
# Notes:
#   - Nonlinearity (linear vs ns(df=3..6)) is tried via spatial CV.
#   - Transform set (x, x2, x3, log1p, logshift) is used only for elastic-net screening.
#   - All base predictors are forced in the final model; no variable selection.
# ============================================================================ #

suppressPackageStartupMessages({
  library(dplyr); library(tidyr); library(purrr); library(readr)
  library(glmnet); library(Matrix); library(splines); library(stringr)
  library(INLA)
})

stopifnot(exists("dat"), exists("fdis_col"), exists("predictor_cols"),
          exists("coords_xy"), exists("mesh"), exists("spde"), exists("A"))

set.seed(123)
y <- dat[[fdis_col]]
ses_name <- "ses_mpd"
results_path <- "model_selection_runs_fdis.csv"

# ---- spatial folds (same across all configs) ---------------------------------
kfold <- 5
set.seed(1001)
km <- stats::kmeans(coords_xy, centers = kfold, nstart = 20)
foldid_global <- km$cluster  # spatially compact folds

# ---------- helpers ------------------------------------------------------------
zscore_vec <- function(x) as.numeric(scale(x)[,1])

impute_cols <- function(D){
  for (nm in names(D)){
    v <- D[[nm]]
    v[!is.finite(v)] <- NA_real_
    if (anyNA(v)) {
      m <- mean(v, na.rm=TRUE); if (!is.finite(m)) m <- 0
      v[is.na(v)] <- m
    }
    D[[nm]] <- v
  }
  D
}

# winsorize controls clipping only; always standardize (z-score)
make_z_table <- function(vars, winsorize=TRUE){
  Z <- lapply(vars, function(v){
    z <- zscore_vec(dat[[v]])
    if (winsorize) z <- pmin(pmax(z, -2), 2)
    z
  })
  Z <- as.data.frame(setNames(Z, vars), check.names = FALSE)
  impute_cols(Z)
}

# transforms for screening on z
build_transforms <- function(x, base){
  out <- list()
  out[[paste0(base,"__lin")]] <- x
  out[[paste0(base,"__x2")]]  <- x^2
  out[[paste0(base,"__x3")]]  <- x^3
  if (all(is.finite(x)) && min(x, na.rm=TRUE) > -1) {
    out[[paste0(base,"__log1p")]] <- log(x + 1)
  }
  out[[paste0(base,"__logshift")]] <- log(x - min(x, na.rm=TRUE) + 1)
  as.data.frame(out, check.names = FALSE)
}

# pairwise linear×linear interactions on z
make_interactions <- function(Z){
  vars <- colnames(Z)
  if (length(vars) < 2) return(NULL)
  combos <- t(utils::combn(vars, 2))
  out <- setNames(vector("list", nrow(combos)), apply(combos, 1, function(p)
    paste0("int__", p[1], "__x__", p[2])
  ))
  for (i in seq_len(nrow(combos))) {
    v1 <- combos[i,1]; v2 <- combos[i,2]
    out[[i]] <- Z[[v1]] * Z[[v2]]
  }
  as.data.frame(out, check.names = FALSE)
}

is_bad <- function(col) {
  all(!is.finite(col)) || sd(col, na.rm=TRUE) <= .Machine$double.eps
}

append_csv_row <- function(df_row, path){
  stopifnot(nrow(df_row)==1)
  if (!file.exists(path)) {
    write.csv(df_row, path, row.names = FALSE)
  } else {
    suppressWarnings(
      write.table(df_row, file = path, sep = ",", row.names = FALSE,
                  col.names = FALSE, append = TRUE)
    )
  }
}

# ns basis using TRAIN-derived knots (no leakage)
ns_with_train_knots <- function(x_all, x_train, df, base){
  proto <- splines::ns(x_train, df = df)
  B <- splines::ns(
    x_all,
    knots          = attr(proto, "knots"),
    Boundary.knots = attr(proto, "Boundary.knots"),
    intercept      = FALSE
  )
  colnames(B) <- paste0(base, "__ns", seq_len(ncol(B)))
  B
}

# CV scoring for a given spec (list var -> linear or spline(df)), Z, and interactions
cv_score_spec <- function(spec, Z, int_pairs, foldid){
  kfold <- max(foldid)
  fold_metrics <- lapply(seq_len(kfold), function(fold){
    train_idx <- which(foldid != fold); test_idx <- which(foldid == fold)

    # Build fixed-effect design (train-based knots for splines)
    cols <- list(Intercept = rep(1, nrow(Z)))
    for (v in names(spec)){
      x_all <- Z[[v]]; s <- spec[[v]]
      if (is.null(s) || s$type=="linear"){
        cols[[v]] <- x_all
      } else {
        B <- ns_with_train_knots(x_all, x_all[train_idx], s$df, v)
        cols[[paste0(v,"__ns")]] <- B
      }
    }
    # interactions (always linear)
    if (length(int_pairs)){
      for (p in int_pairs){
        nm <- paste0("int__", p[1], "__x__", p[2])
        cols[[nm]] <- Z[[p[1]]] * Z[[p[2]]]
      }
    }
    Xfix <- as.data.frame(do.call(cbind, cols), check.names = FALSE)

    y_all <- y; y_all[test_idx] <- NA
    stk <- INLA::inla.stack(
      data    = list(y = y_all),
      A       = list(A, 1),
      effects = list(s = 1:spde$n.spde, Xfix),
      tag     = "cv"
    )
    rhs  <- paste(colnames(Xfix), collapse = " + ")
    form <- stats::as.formula(paste("y ~ -1 +", rhs, "+ f(s, model = spde)"))

    fit  <- INLA::inla(
      form, family="gaussian",
      data = INLA::inla.stack.data(stk),
      control.predictor = list(A = INLA::inla.stack.A(stk), compute = TRUE),
      control.compute   = list(dic = FALSE, waic = FALSE),
      control.inla = list(strategy = "gaussian", int.strategy = "eb", diagonal = 1e-3),
      verbose = FALSE
    )
    pred <- fit$summary.fitted.values$mean[INLA::inla.stack.index(stk, "cv")$data][test_idx]
    obs  <- y[test_idx]
    data.frame(
      Fold = fold,
      RMSE = sqrt(mean((obs - pred)^2, na.rm=TRUE)),
      MAE  = mean(abs(obs - pred), na.rm=TRUE),
      R2   = 1 - sum((obs - pred)^2, na.rm=TRUE) /
                 sum((obs - mean(obs, na.rm=TRUE))^2, na.rm=TRUE)
    )
  }) %>% dplyr::bind_rows()

  fold_metrics %>%
    summarise(RMSE_mean = mean(RMSE), RMSE_sd = sd(RMSE),
              MAE_mean  = mean(MAE),  MAE_sd  = sd(MAE),
              R2_mean   = mean(R2),   R2_sd   = sd(R2))
}

# Final full-data fit (returns model and its stack index)
fit_full_spec <- function(spec, Z, int_pairs){
  cols <- list(Intercept = rep(1, nrow(Z)))
  for (v in names(spec)){
    x <- Z[[v]]; s <- spec[[v]]
    if (is.null(s) || s$type=="linear"){
      cols[[v]] <- x
    } else {
      B <- splines::ns(x, df = s$df)
      colnames(B) <- paste0(v,"__ns", seq_len(ncol(B)))
      cols[[paste0(v,"__ns")]] <- B
    }
  }
  if (length(int_pairs)){
    for (p in int_pairs){
      nm <- paste0("int__", p[1], "__x__", p[2])
      cols[[nm]] <- Z[[p[1]]] * Z[[p[2]]]
    }
  }
  Xfix <- as.data.frame(do.call(cbind, cols), check.names = FALSE)

  stk <- INLA::inla.stack(
    data    = list(y = y),
    A       = list(A, 1),
    effects = list(s = 1:spde$n.spde, Xfix),
    tag     = "final"
  )
  rhs  <- paste(colnames(Xfix), collapse = " + ")
  form <- stats::as.formula(paste("y ~ -1 +", rhs, "+ f(s, model = spde)"))

  fit <- INLA::inla(
    form, family="gaussian",
    data = INLA::inla.stack.data(stk),
    control.predictor = list(A = INLA::inla.stack.A(stk), compute = TRUE),
    control.compute   = list(dic = TRUE, waic = TRUE, cpo = FALSE),
    control.inla = list(strategy = "gaussian", int.strategy = "eb", diagonal = 1e-3),
    verbose = FALSE
  )
  idx_final <- INLA::inla.stack.index(stk, "final")$data
  list(fit = fit, idx = idx_final)
}

# ----- one configuration end-to-end -------------------------------------------
run_one_config <- function(cfg_id, winsorize, use_interactions, include_ses, foldid = foldid_global){

  # predictors for this run
  preds <- predictor_cols
  if (!include_ses) preds <- setdiff(preds, ses_name)

  # 1) Z table
  Z <- make_z_table(preds, winsorize = winsorize)

  # 2) FORCE ALL VARIABLES (no variable selection)
  base_selected <- preds

  # 3) Screening for nonlinearity hints (transforms only)
  X_big <- purrr::map_dfc(preds, ~ build_transforms(Z[[.x]], .x))
  keep <- !vapply(X_big, is_bad, logical(1))
  X_big <- X_big[, keep, drop = FALSE]
  X_sp  <- Matrix::Matrix(as.matrix(X_big), sparse = TRUE)

  # Guard: empty design
  if (is.null(dim(X_sp)) || ncol(X_sp) == 0) {
    return(data.frame(
      timestamp = format(Sys.time(), "%Y-%m-%d %H:%M:%S"),
      config_id = cfg_id,
      winsorize = winsorize,
      interactions = use_interactions,
      include_ses_mpd = include_ses,
      alpha_chosen = NA, n_terms_elasticnet = 0,
      n_base_vars = length(preds), base_vars = paste(preds, collapse=";"),
      n_interactions = 0, interactions_list = "",
      shapes = "",
      CV_RMSE_mean = NA, CV_RMSE_sd = NA,
      CV_R2_mean = NA,  CV_R2_sd = NA,
      WAIC = NA, DIC = NA,
      in_sample_RMSE = NA, in_sample_R2 = NA,
      status = "error: empty_design_after_filtering",
      stringsAsFactors = FALSE
    ))
  }

  # 4) Elastic-net CV ONLY for hints
  alphas <- c(1.0, 0.5, 0.2)
  fits <- lapply(alphas, function(a)
    glmnet::cv.glmnet(X_sp, y, family="gaussian", alpha=a, foldid=foldid,
                      standardize=TRUE, intercept=TRUE)
  )
  best_idx <- which.min(sapply(fits, function(f) min(f$cvm)))
  cvfit <- fits[[best_idx]]

  coefs <- coef(cvfit, s = "lambda.1se")
  sel_idx <- which(as.numeric(coefs) != 0)
  sel_names <- setdiff(rownames(coefs)[sel_idx], "(Intercept)")
  if (!length(sel_names)) {
    coefs <- coef(cvfit, s = "lambda.min")
    sel_idx <- which(as.numeric(coefs) != 0)
    sel_names <- setdiff(rownames(coefs)[sel_idx], "(Intercept)")
  }

  base_of <- function(nm) sub("^int__|__x__.*$|__.*$", "", nm)
  is_int  <- function(nm) grepl("^int__", nm)
  is_nl   <- function(nm) grepl("__x2|__x3|__log1p|__logshift", nm)

  # Interactions screening (optional)
  int_pairs <- list()
  alpha_out <- alphas[best_idx]
  n_terms_en <- length(sel_names)

  if (use_interactions){
    X_int <- make_interactions(Z)
    if (!is.null(X_int) && ncol(X_int) > 0) {
      X_big_int <- cbind(X_big, X_int)
      keep_int <- !vapply(X_big_int, is_bad, logical(1))
      X_big_int <- X_big_int[, keep_int, drop = FALSE]
      X_sp_int <- Matrix::Matrix(as.matrix(X_big_int), sparse = TRUE)

      fits_int <- lapply(alphas, function(a)
        glmnet::cv.glmnet(X_sp_int, y, family="gaussian", alpha=a, foldid=foldid,
                          standardize=TRUE, intercept=TRUE)
      )
      best_idx_int <- which.min(sapply(fits_int, function(f) min(f$cvm)))
      cvfit_int <- fits_int[[best_idx_int]]

      coefs_int <- coef(cvfit_int, s = "lambda.1se")
      sel_idx_int <- which(as.numeric(coefs_int) != 0)
      sel_names_int <- setdiff(rownames(coefs_int)[sel_idx_int], "(Intercept)")
      if (!length(sel_names_int)) {
        coefs_int <- coef(cvfit_int, s = "lambda.min")
        sel_idx_int <- which(as.numeric(coefs_int) != 0)
        sel_names_int <- setdiff(rownames(coefs_int)[sel_idx_int], "(Intercept)")
      }

      # record EN stats from the interaction screen
      alpha_out  <- alphas[best_idx_int]
      n_terms_en <- length(sel_names_int)
      sel_names  <- sel_names_int

      # Extract selected interactions
      int_names <- sel_names_int[is_int(sel_names_int)]
      if (length(int_names)){
        for (nm in int_names){
          m <- stringr::str_match(nm, "^int__(.+?)__x__(.+)$")
          if (!any(is.na(m))){
            v1 <- m[2]; v2 <- m[3]
            if (all(c(v1, v2) %in% colnames(Z))) {
              int_pairs[[length(int_pairs)+1]] <- c(v1, v2)
            }
          }
        }
      }
    }
  }

  # De-duplicate & canonicalize interaction pairs
  if (length(int_pairs)) {
    int_pairs <- unique(lapply(int_pairs, function(p) sort(p)))
  }

  # Nonlinearity candidates
  if (length(sel_names) > 0) {
    nl_candidates <- intersect(base_selected, unique(base_of(sel_names[is_nl(sel_names)])))
  } else {
    nl_candidates <- base_selected
  }
  if (length(nl_candidates) == 0) nl_candidates <- base_selected

  # 5) Per-variable nonlinearity choice via spatial CV
  spec <- setNames(lapply(base_selected, function(v) list(type="linear")), base_selected)
  df_grid <- 3:6

  for (v in nl_candidates){
    if (sd(Z[[v]], na.rm=TRUE) <= .Machine$double.eps) next
    base_score <- cv_score_spec(spec, Z, int_pairs, foldid)
    tried_rows <- lapply(df_grid, function(dfk){
      spec_try <- spec; spec_try[[v]] <- list(type="spline", df=dfk)
      sc <- cv_score_spec(spec_try, Z, int_pairs, foldid)
      data.frame(df = dfk,
                 RMSE_mean = as.numeric(sc$RMSE_mean),
                 MAE_mean  = as.numeric(sc$MAE_mean),
                 R2_mean   = as.numeric(sc$R2_mean))
    })
    tried <- dplyr::bind_rows(tried_rows)
    df_best <- tried$df[ which.min(tried$RMSE_mean) ]
    if (min(tried$RMSE_mean) + 1e-6 < as.numeric(base_score$RMSE_mean)) {
      spec[[v]] <- list(type="spline", df = as.integer(df_best))
    }
  }

  # 6) Final full-data fit + CV metrics
  ff <- fit_full_spec(spec, Z, int_pairs)
  fit_final <- ff$fit
  idx_final <- ff$idx
  cv_final  <- cv_score_spec(spec, Z, int_pairs, foldid)

  fitted_final <- fit_final$summary.fitted.values$mean[idx_final]
  in_rmse <- sqrt(mean((y - fitted_final)^2, na.rm=TRUE))
  in_r2   <- 1 - var(y - fitted_final, na.rm=TRUE) / var(y, na.rm=TRUE)

  nl_map <- paste0(
    vapply(names(spec), function(v){
      if (!is.null(spec[[v]]) && spec[[v]]$type=="spline") paste0(v, ":ns", spec[[v]]$df)
      else paste0(v, ":lin")
    }, character(1L)), collapse = ";"
  )
  ints_str <- if (length(int_pairs)) paste0(vapply(int_pairs, function(p) paste(p, collapse="×"), ""), collapse=";") else ""

  data.frame(
    timestamp = format(Sys.time(), "%Y-%m-%d %H:%M:%S"),
    config_id = cfg_id,
    winsorize = winsorize,
    interactions = use_interactions,
    include_ses_mpd = include_ses,
    alpha_chosen = alpha_out,
    n_terms_elasticnet = n_terms_en,
    n_base_vars = length(base_selected),
    base_vars = paste(base_selected, collapse=";"),
    n_interactions = length(int_pairs),
    interactions_list = ints_str,
    shapes = nl_map,
    CV_RMSE_mean = as.numeric(cv_final$RMSE_mean),
    CV_RMSE_sd   = as.numeric(cv_final$RMSE_sd),
    CV_R2_mean   = as.numeric(cv_final$R2_mean),
    CV_R2_sd     = as.numeric(cv_final$R2_sd),
    WAIC = as.numeric(fit_final$waic$waic),
    DIC  = as.numeric(fit_final$dic$dic),
    in_sample_RMSE = in_rmse,
    in_sample_R2   = in_r2,
    status = "ok",
    stringsAsFactors = FALSE
  )
}

# ---------- run the configurations with a progress bar ------------------------
configs <- list(
  # list(id="C1_noInt_winz_noSES",    winsorize=TRUE,  interactions=FALSE, include_ses=FALSE),
  # list(id="C2_noInt_unclip_noSES",  winsorize=FALSE, interactions=FALSE, include_ses=FALSE),
  # list(id="C3_Int_winz_noSES",      winsorize=TRUE,  interactions=TRUE,  include_ses=FALSE),
  list(id="C4_noInt_unclip_withMPD", winsorize=FALSE, interactions=FALSE, include_ses=TRUE)
)

pb <- txtProgressBar(min=0, max=length(configs), style=3)
for (i in seq_along(configs)){
  cfg <- configs[[i]]
  cat(sprintf("\n--- Running %s (winsorize=%s, interactions=%s, include_ses_mpd=%s) ---\n",
              cfg$id, cfg$winsorize, cfg$interactions, cfg$include_ses))
  res_row <- tryCatch(
    run_one_config(cfg$id, cfg$winsorize, cfg$interactions, cfg$include_ses, foldid_global),
    error = function(e){
      data.frame(
        timestamp = format(Sys.time(), "%Y-%m-%d %H:%M:%S"),
        config_id = cfg$id,
        winsorize = cfg$winsorize,
        interactions = cfg$interactions,
        include_ses_mpd = cfg$include_ses,
        alpha_chosen = NA,
        n_terms_elasticnet = NA,
        n_base_vars = NA,
        base_vars = NA,
        n_interactions = NA,
        interactions_list = NA,
        shapes = NA,
        CV_RMSE_mean = NA, CV_RMSE_sd = NA,
        CV_R2_mean = NA,  CV_R2_sd  = NA,
        WAIC = NA, DIC = NA,
        in_sample_RMSE = NA, in_sample_R2 = NA,
        status = paste0("error: ", conditionMessage(e)),
        stringsAsFactors = FALSE
      )
    }
  )
  append_csv_row(res_row, results_path)
  print(res_row[, c("config_id","CV_RMSE_mean","CV_R2_mean","WAIC","DIC","status")])
  setTxtProgressBar(pb, i)
}
close(pb)
cat(sprintf("\nResults appended to: %s\n", results_path))
# ============================================================================ #
```


```{r}
# --- Publication plots: standardized partial effects for C4 only ----
suppressPackageStartupMessages({
  library(dplyr);  library(tidyr);  library(purrr);  library(readr)
  library(INLA);   library(splines); library(stringr); library(ggplot2); library(gt)
  library(scales); library(grid)
})

stopifnot(exists("dat"), exists("fdis_col"), exists("mesh"), exists("spde"), exists("A"))

`%||%` <- function(a,b) if (is.null(a) || (length(a)==1 && is.na(a))) b else a
to_bool <- function(x){
  if (is.logical(x)) return(x)
  if (is.numeric(x)) return(x != 0)
  x <- tolower(as.character(x))
  x %in% c("true","t","yes","y","1")
}

# ------------------------- helpers --------------------------------------------
zscore_vec <- function(x) as.numeric(scale(x)[,1])

impute_cols <- function(D){
  for (nm in names(D)){
    v <- D[[nm]]; v[!is.finite(v)] <- NA_real_
    if (anyNA(v)) { m <- mean(v, na.rm = TRUE); if (!is.finite(m)) m <- 0; v[is.na(v)] <- m }
    D[[nm]] <- v
  }
  D
}

make_z_table <- function(vars, winsorize=TRUE){
  Z <- lapply(vars, function(v){
    z <- zscore_vec(dat[[v]])
    if (winsorize) z <- pmin(pmax(z, -2), 2)
    z
  })
  impute_cols(as.data.frame(setNames(Z, vars), check.names = FALSE))
}

parse_shapes <- function(s){
  s <- trimws(as.character(s) %||% ""); if (s == "") return(list())
  parts <- unlist(strsplit(s, ";"))
  res <- vector("list", length(parts)); names(res) <- character(length(parts))
  k <- 0L
  for (p in parts){
    kv <- unlist(strsplit(p, ":"))
    if (length(kv) != 2) next
    k <- k + 1L
    v <- kv[1]; sh <- kv[2]
    res[[k]] <- if (grepl("^ns\\d+$", sh)) list(type="spline", df=as.integer(sub("^ns","",sh)))
                else                         list(type="linear")
    names(res)[k] <- v
  }
  res[seq_len(k)]
}

parse_interactions <- function(s){
  s <- trimws(as.character(s) %||% ""); if (s == "") return(list())
  s <- gsub("×", "×", s, fixed = TRUE)
  items <- unlist(strsplit(s, ";"))
  pairs <- list()
  for (it in items){
    ab <- unlist(strsplit(it, "×", fixed = TRUE))
    if (length(ab)==2) pairs[[length(pairs)+1]] <- ab
  }
  pairs
}

build_Xfix <- function(Z, spec, int_pairs){
  cols <- list(Intercept = rep(1, nrow(Z))); protos <- list()
  for (v in names(spec)){
    x <- Z[[v]]; s <- spec[[v]]
    if (is.null(s) || s$type=="linear"){
      cols[[v]] <- x
    } else {
      proto <- splines::ns(x, df = s$df)
      colnames(proto) <- paste0(v,"__ns", seq_len(ncol(proto)))
      cols[[paste0(v,"__ns")]] <- proto
      protos[[v]] <- proto
    }
  }
  if (length(int_pairs)){
    for (p in int_pairs){
      nm <- paste0("int__", p[1], "__x__", p[2])
      if (all(p %in% colnames(Z))) cols[[nm]] <- Z[[p[1]]] * Z[[p[2]]]
    }
  }
  list(Xfix = as.data.frame(do.call(cbind, cols), check.names = FALSE), protos = protos)
}

fit_inla_spec <- function(Xfix, y){
  stk <- INLA::inla.stack(
    data    = list(y = y),
    A       = list(A, 1),
    effects = list(s = 1:spde$n.spde, Xfix),
    tag     = "final"
  )
  rhs  <- paste(colnames(Xfix), collapse = " + ")
  form <- as.formula(paste("y ~ -1 +", rhs, "+ f(s, model = spde)"))
  fit <- INLA::inla(
    form, family="gaussian",
    data = INLA::inla.stack.data(stk),
    control.predictor = list(A = INLA::inla.stack.A(stk), compute = TRUE),
    control.compute   = list(dic = TRUE, waic = TRUE),
    control.inla = list(strategy = "gaussian", int.strategy = "eb", diagonal = 1e-3),
    verbose = FALSE
  )
  list(fit=fit, idx=INLA::inla.stack.index(stk, "final")$data)
}

compute_effects <- function(var, Z, spec, fit, protos, grid_len = 400){
  sfix <- fit$summary.fixed
  beta_mean <- setNames(sfix$mean, rownames(sfix))
  beta_lo   <- setNames(sfix$`0.025quant`, rownames(sfix))
  beta_hi   <- setNames(sfix$`0.975quant`, rownames(sfix))
  x <- Z[[var]]

  eff_at <- function(xvec, which = c("mean","lo","hi")){
    which <- match.arg(which)
    bb <- switch(which, mean=beta_mean, lo=beta_lo, hi=beta_hi)
    if (!is.null(spec[[var]]) && spec[[var]]$type=="spline"){
      proto <- protos[[var]]
      B <- splines::ns(xvec,
                       knots = attr(proto,"knots"),
                       Boundary.knots = attr(proto,"Boundary.knots"),
                       intercept = FALSE)
      colnames(B) <- colnames(proto)
      as.numeric(B %*% bb[colnames(B)])
    } else {
      as.numeric(xvec * bb[var])
    }
  }

  # AME (+1 SD) with clipping to spline bounds
  if (!is.null(spec[[var]]) && spec[[var]]$type=="spline"){
    bk <- attr(protos[[var]], "Boundary.knots")
    x_plus <- pmin(pmax(x + 1, bk[1]), bk[2])
  } else x_plus <- x + 1
  ame <- mean(eff_at(x_plus, "mean") - eff_at(x, "mean"), na.rm = TRUE)

  # robust grid when var has ~0 range
  r <- range(x, na.rm = TRUE)
  gx <- if (!is.finite(diff(r)) || diff(r) == 0) seq(-2, 2, length.out = grid_len)
        else seq(r[1], r[2], length.out = grid_len)

  grid_eff <- data.frame(
    x = gx,
    mean  = eff_at(gx, "mean"),
    lower = eff_at(gx, "lo"),
    upper = eff_at(gx, "hi")
  )
  list(AME_1SD = ame, grid = grid_eff)
}

# -------------------- configs & output ----------------------------------------
runs <- readr::read_csv("model_selection_runs_fdis.csv", show_col_types = FALSE)

runs <- runs %>%
  mutate(
    config_id = as.character(config_id),
    shapes    = as.character(shapes %||% ""),
    interactions_list = as.character(interactions_list %||% ""),
    winsorize = to_bool(winsorize)
  ) %>%
  filter(config_id == "C4_noInt_unclip_withMPD")

if (nrow(runs) == 0) stop("C4_noInt_unclip_withMPD not found in model_selection_runs_fdis.csv")

outdir <- "output/plots/partial_effects_configs_fdis"
if (!dir.exists(outdir)) dir.create(outdir, recursive = TRUE)

pretty <- c(
  temp_mean="Temperature", rh_mean="Relative humidity", ruggedness="Ruggedness",
  hfp="Human footprint", veg_pc1="Vegetation PC1", ses_mpd="SES(MPD)"
)
col_pos <- "#2166AC"; col_neg <- "#B2182B"

y <- dat[[fdis_col]]
fdis_sd <- sd(y, na.rm = TRUE)

# -------------------- Process C4 ----------------------------------------------
c4_run <- runs %>% slice(1)

spec_c4 <- parse_shapes(c4_run$shapes[1])
vars_c4 <- names(spec_c4)
int_pairs_c4 <- parse_interactions(c4_run$interactions_list[1])
Z_c4 <- make_z_table(vars = vars_c4, winsorize = c4_run$winsorize[1])

dd_c4 <- build_Xfix(Z_c4, spec_c4, int_pairs_c4)
ff_c4 <- fit_inla_spec(dd_c4$Xfix, y)
fit_c4 <- ff_c4$fit
protos_c4 <- dd_c4$protos

# Compute effects
eff_list_c4 <- lapply(vars_c4, function(v) compute_effects(v, Z_c4, spec_c4, fit_c4, protos_c4))
names(eff_list_c4) <- vars_c4

# AME table
ame_c4 <- tibble(
  config_id = c4_run$config_id,
  variable  = vars_c4,
  label     = unname(pretty[vars_c4]) %||% vars_c4,
  shape     = vapply(vars_c4, function(v) if (!is.null(spec_c4[[v]]) && spec_c4[[v]]$type=="spline")
                              paste0("ns", spec_c4[[v]]$df) else "lin", character(1)),
  AME_1SD   = vapply(eff_list_c4, \(a) a$AME_1SD, numeric(1)),
  AME_sd    = AME_1SD / fdis_sd,
  AME_pct_FDIS_SD = 100 * AME_1SD / fdis_sd
) %>%
  arrange(desc(abs(AME_sd))) %>%
  mutate(
    AME_1SD = round(AME_1SD, 5),
    AME_sd  = round(AME_sd, 5),
    AME_pct_FDIS_SD = round(AME_pct_FDIS_SD, 2)
  )

# Save AME table
out_csv <- file.path(outdir, "AME_C4_standardized_fdis.csv")
readr::write_csv(ame_c4, out_csv)
cat("Saved AME table:", out_csv, "\n")

# Display table
ame_c4 %>%
  gt() %>%
  tab_header(title = "Average Marginal Effect (+1 SD; raw & standardized) - C4") %>%
  cols_label(
    config_id = "Configuration", variable = "Variable (name)", label = "Variable (label)",
    shape = "Shape", AME_1SD = "AME (+1 SD) [FDIS units]",
    AME_sd = "Standardized AME (FDIS SDs)", AME_pct_FDIS_SD = "AME as % of SD(FDIS)"
  )

# Prepare curves
pred_order_c4 <- ame_c4 %>% pull(variable)

curves_c4 <- bind_rows(lapply(vars_c4, function(v){
  g <- eff_list_c4[[v]]$grid
  g$variable <- v
  g$label <- unname(pretty[v]) %||% v
  g
})) %>%
  mutate(
    mean  = mean  / fdis_sd,
    lower = lower / fdis_sd,
    upper = upper / fdis_sd
  )

dir_map_c4 <- ame_c4 %>%
  transmute(variable, Direction = ifelse(AME_sd > 0, "Positive", "Negative"))
curves_c4 <- curves_c4 %>% left_join(dir_map_c4, by = "variable")

# Quantile rugs with 101 bins
n_quant <- 101
probs <- seq(0, 1, length.out = n_quant)

rugs_q <- bind_rows(lapply(vars_c4, function(v) {
  if (!v %in% names(Z_c4)) return(NULL)
  qx <- quantile(Z_c4[[v]], probs = probs, na.rm = TRUE)
  tibble(x = as.numeric(unique(qx[is.finite(qx)])),
         label = unname(pretty[v]) %||% v)
})) %>%
  mutate(label = factor(label, levels = unname(pretty[pred_order_c4]) %||% pred_order_c4))

# Custom x-breaks: 3-4 tick labels per facet
breaks_3_4 <- function(lims) {
  rng <- range(lims, finite = TRUE)
  br <- scales::breaks_extended(n = 4)(rng)
  br <- br[br >= rng[1] & br <= rng[2]]
  if (length(br) > 4) br <- br[round(seq(1, length(br), length.out = 4))]
  if (length(br) < 3) {
    br <- scales::pretty_breaks(n = 3)(rng)
    br <- br[br >= rng[1] & br <= rng[2]]
    if (length(br) > 4) br <- br[round(seq(1, length(br), length.out = 4))]
  }
  br
}

# Create plot
p_c4 <- ggplot(curves_c4, aes(x = x)) +
  geom_hline(yintercept = 0, linewidth = 0.5, linetype = "dashed", color = "grey70") +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = Direction), alpha = 0.12) +
  geom_line(aes(y = mean, color = Direction), linewidth = 1.3) +
  { if (nrow(rugs_q)) geom_rug(
      data = rugs_q, inherit.aes = FALSE,
      aes(x = x), sides = "b",
      color = "black", alpha = 0.55, length = unit(0.02, "npc")
    ) else NULL } +
  scale_color_manual(values = c("Positive" = col_pos, "Negative" = col_neg), guide = "none") +
  scale_fill_manual(values = c("Positive" = col_pos, "Negative" = col_neg), guide = "none") +
  scale_y_continuous(breaks = scales::breaks_extended(n = 4)) +
  scale_x_continuous(breaks = breaks_3_4) +
  facet_wrap(~ factor(label, levels = unname(pretty[pred_order_c4]) %||% pred_order_c4),
             scales = "free_x", ncol = 3) +
  labs(title = "C4_noInt_unclip_withMPD",
       x = "Predictor (z-score)", y = "Partial effect on FDIS (SD units)") +
  theme_classic(base_size = 18) +
  theme(
    strip.background = element_blank(),
    strip.text = element_text(size = 16, face = "bold"),
    axis.title = element_text(size = 24),
    axis.text  = element_text(size = 20),
    panel.spacing = unit(1.0, "lines"),
    plot.margin = margin(8, 16, 8, 8),
    legend.position = "none"
  ) +
  coord_cartesian(clip = "off")

ggsave(file.path(outdir, "partial_effects_C4_noInt_unclip_withMPD.png"),
       p_c4, width = 7, height = 11, dpi = 600, units = "in", bg = "white")
```


```{r}
# --- Robust Variance Decomposition for Selected Model (C4) ---
stopifnot(exists("fit_c4"), exists("dd_c4"), exists("A"), exists("dat"))

# 0) Helpers
fmt <- function(x) formatC(x, digits = 6, format = "f")
pct <- function(x) sprintf("%.2f%%", 100 * x)

# 1) Extract Fixed Effects (Betas)
# We must align betas with the design matrix columns (which include splines)
beta <- fit_c4$summary.fixed$mean
names(beta) <- rownames(fit_c4$summary.fixed)

# Use the Design Matrix (dd_c4$Xfix) created in the previous block
# This handles the Intercept and any Spline expansions automatically
Xfix_mat <- as.matrix(dd_c4$Xfix)

# Calculate Fixed Component (X * beta)
# Ensure we select betas in the exact order of the matrix columns
fixed_pred <- as.numeric(Xfix_mat %*% beta[colnames(Xfix_mat)])

# 2) Calculate Spatial Component at data locations (Projected)
spatial_at_data <- as.numeric(A %*% fit_c4$summary.random$s$mean)

# 3) One fitted definition for EVERYTHING
fitted_sum <- fixed_pred + spatial_at_data

# 4) Common mask to prevent NA / row-set drift
y_full <- dat[[fdis_col]]
keep   <- is.finite(y_full) & is.finite(fixed_pred) & is.finite(spatial_at_data)

y_k     <- y_full[keep]
fixed_k <- fixed_pred[keep]
spat_k  <- spatial_at_data[keep]
fit_k   <- fitted_sum[keep]

# Optional: Fitted from INLA stack (for validation only)
if(exists("ff_c4") && !is.null(ff_c4$idx)) {
  sfit_all <- fit_c4$summary.fitted.values$mean[ff_c4$idx]
  sfit_k   <- sfit_all[keep]
} else {
  sfit_k <- NULL 
}

# 5) Variances and covariance on the SAME rows
var_y    <- var(y_k)
var_fix  <- var(fixed_k)
var_spat <- var(spat_k)
cov_fs   <- cov(fixed_k, spat_k)
cor_fs   <- cor(fixed_k, spat_k)

# 6) Decomposition
var_fit     <- var(fit_k)
var_parts   <- var_fix + var_spat + 2 * cov_fs
R2_total    <- var_fit / var_y
R2_fixed    <- var_fix  / var_y
R2_spatial  <- var_spat / var_y
R2_shared   <- (2 * cov_fs) / var_y
R2_residual <- 1 - R2_total

# 7) Reporting
cat("=== VARIANCE DECOMPOSITION (Selected Model C4) ===\n")
cat(sprintf("N_total = %d | N_used = %d | dropped = %d\n",
            length(y_full), length(y_k), sum(!keep)))

cat("\n--- Variances and covariance (on used rows) ---\n")
cat(sprintf("var(y)                     = %s\n", fmt(var_y)))
cat(sprintf("var(Fixed)                 = %s\n", fmt(var_fix)))
cat(sprintf("var(Spatial)               = %s\n", fmt(var_spat)))
cat(sprintf("cov(Fixed, Spatial)        = %s\n", fmt(cov_fs)))
cat(sprintf("cor(Fixed, Spatial)        = %.6f\n", cor_fs))
cat(sprintf("var(Fixed + Spatial)       = %s\n", fmt(var_fit)))
cat(sprintf("var(F)+var(S)+2*cov(F,S)   = %s\n", fmt(var_parts)))

if(!is.null(sfit_k)) {
  cat(sprintf("max| (F+S) - fitted_stack | = %.3e (Check vs INLA internal fit)\n",
              max(abs(fit_k - sfit_k), na.rm = TRUE)))
}

cat("\n--- R^2 components (fractions of var(y)) ---\n")
cat(sprintf("R2_total                   = %s  (%s)\n", fmt(R2_total),    pct(R2_total)))
cat(sprintf("R2_fixed                   = %s  (%s)\n", fmt(R2_fixed),    pct(R2_fixed)))
cat(sprintf("R2_spatial                 = %s  (%s)\n", fmt(R2_spatial),  pct(R2_spatial)))
cat(sprintf("R2_shared (2*cov/var(y))   = %s  (%s)\n", fmt(R2_shared),   pct(R2_shared)))
cat(sprintf("R2_residual                = %s  (%s)\n", fmt(R2_residual), pct(R2_residual)))
cat(sprintf("R^2 closure error          = %.3e\n",
            (R2_fixed + R2_spatial + R2_shared + R2_residual) - 1))

# 8) Tidy summary table
report_c4 <- data.frame(
  Metric  = c("R2_total","R2_fixed","R2_spatial","R2_shared","R2_residual"),
  Value   = c(R2_total, R2_fixed, R2_spatial, R2_shared, R2_residual),
  Percent = c(pct(R2_total), pct(R2_fixed), pct(R2_spatial), pct(R2_shared), pct(R2_residual))
)
cat("\n=== R^2 SUMMARY TABLE (C4) ===\n")
print(report_c4, row.names = FALSE)
```
```{r}
# --- COVARIANCE SAFETY CHECK: Spatial vs Non-Spatial Stability ---
# Prerequisites: This relies on 'fit_c4' and 'dd_c4' from the previous "Publication plots" chunk.

suppressPackageStartupMessages({
  library(ggplot2); library(dplyr); library(broom); library(tibble)
})

if (!exists("fit_c4") || !exists("dd_c4")) {
  stop("Please run the 'Publication plots' chunk first to generate the C4 model object.")
}

cat("--- Running Stability Diagnosis ---\n")

# 1. Prepare Data
# We use the EXACT design matrix (Xfix) from C4 so we compare apples-to-apples (same splines)
X_design <- dd_c4$Xfix
y_response <- dat[[fdis_col]] 

# 2. Fit Non-Spatial OLS Benchmark
# We use '0 +' because X_design already contains an "(Intercept)" column
model_ols_c4 <- lm(y_response ~ 0 + ., data = X_design)

# 3. Extract OLS Results
res_ols <- broom::tidy(model_ols_c4, conf.int = TRUE) %>%
  dplyr::select(term, estimate, conf.low, conf.high) %>%
  mutate(Model = "OLS (Non-Spatial)")

# 4. Extract Spatial (INLA) Results
# We grab the fixed effects from the fitted INLA object
res_inla <- fit_c4$summary.fixed %>%
  rownames_to_column("term") %>%
  dplyr::select(term, estimate = mean, conf.low = `0.025quant`, conf.high = `0.975quant`) %>%
  mutate(Model = "INLA (Spatial C4)")

# 5. Combine and Flag "Sign Flips"
comparison <- bind_rows(res_ols, res_inla) %>%
  filter(term != "(Intercept)") %>% # Hide intercept (scale is too different)
  filter(term != "Intercept")     # Handle potential naming diffs
  
# Check for sign flips (where OLS and INLA have opposite signs and both are somewhat significant)
check_flip <- comparison %>%
  select(term, Model, estimate) %>%
  pivot_wider(names_from = Model, values_from = estimate) %>%
  mutate(
    Sign_Flip = sign(`OLS (Non-Spatial)`) != sign(`INLA (Spatial C4)`),
    Magnitude_Change_Pct = abs(`INLA (Spatial C4)` - `OLS (Non-Spatial)`) / abs(`OLS (Non-Spatial)`) * 100
  )

print(check_flip)

# 6. Plot the "Forest Plot"
p_safe <- ggplot(comparison, aes(x = estimate, y = term, color = Model)) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray50") +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), 
                 position = position_dodge(width = 0.6), height = 0.2) +
  geom_point(position = position_dodge(width = 0.6), size = 3) +
  scale_color_manual(values = c("INLA (Spatial C4)" = "#2166AC", "OLS (Non-Spatial)" = "#B2182B")) +
  labs(
    title = "Stability Check: Fixed Effects vs Spatial Confounding",
    subtitle = "If intervals overlap and signs are consistent, the Negative Covariance is harmless.",
    x = "Coefficient Estimate (Standardized)",
    y = "Predictor Term (Spline Basis / Linear)"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    axis.text.y = element_text(size = 10),
    plot.title = element_text(face = "bold")
  )

print(p_safe)

# 7. Automated Verdict for the PI
flips <- check_flip %>% filter(Sign_Flip == TRUE)
```

