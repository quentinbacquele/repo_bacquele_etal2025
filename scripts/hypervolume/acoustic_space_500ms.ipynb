{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import math, pandas and soundsig libraries\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import umap\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = np.load('./data/metadata_500ms.npz', allow_pickle=True)\n",
    "MPS = np.load('./data/X_500ms.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = np.array([len(subarr) for subarr in MPS])\n",
    "min_length = lengths.min()\n",
    "max_length = lengths.max()\n",
    "mean_length = lengths.mean()\n",
    "median_length = np.median(lengths)\n",
    "std_length = lengths.std()\n",
    "\n",
    "print(f\"Min length: {min_length}\")\n",
    "print(f\"Max length: {max_length}\")\n",
    "print(f\"Mean length: {mean_length:.2f}\")\n",
    "print(f\"Median length: {median_length}\")\n",
    "print(f\"Standard Deviation: {std_length:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_length = 9898\n",
    "\n",
    "valid_indices = []\n",
    "filtered_mps = []\n",
    "\n",
    "for i, mps in enumerate(MPS):\n",
    "    if len(mps) == expected_length:\n",
    "        filtered_mps.append(mps)\n",
    "        valid_indices.append(i)\n",
    "\n",
    "print(f\"Removed {len(MPS) - len(filtered_mps)} outlier(s) out of {len(MPS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MPS = np.stack(filtered_mps)\n",
    "print(\"Shape of standardized MPS array:\", MPS.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dict = {key: metadata[key] for key in metadata.files}\n",
    "\n",
    "metadata_df = pd.DataFrame(metadata_dict)\n",
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = metadata_df.iloc[valid_indices].reset_index(drop=True)\n",
    "metadata_df['species'] = metadata_df['filename'].apply(lambda x: '_'.join(x.split('_')[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_counts = metadata_df['species'].value_counts()\n",
    "metadata_df['weights'] = metadata_df['species'].map(species_counts)\n",
    "metadata_df['weights'] = 1.0 / metadata_df['weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = MPS.shape\n",
    "\n",
    "print('Number of samples:', n_samples)\n",
    "print('Number of features:', n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix = MPS.astype(np.float32)\n",
    "weights = np.array(metadata_df['weights'].values, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_matrix = (data_matrix.T * weights) @ data_matrix / np.sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import eig\n",
    "\n",
    "# Eigendecomposition of covariance matrix\n",
    "eig_vals, eig_vecs = eig(cov_matrix)\n",
    "eig_vals = np.real(eig_vals)\n",
    "eig_vecs = np.real(eig_vecs)\n",
    "\n",
    "# Adjusting the eigenvectors (loadings) that are largest in absolute value to be positive\n",
    "max_abs_idx = np.argmax(np.abs(eig_vecs), axis=0)\n",
    "signs = np.sign(eig_vecs[max_abs_idx, range(eig_vecs.shape[0])])\n",
    "eig_vecs = eig_vecs*signs[np.newaxis,:]\n",
    "eig_vecs = eig_vecs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[i,:]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Then, we sort the tuples from the highest to the lowest based on eigenvalues magnitude\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# For further usage\n",
    "eig_vals_sorted = np.array([x[0] for x in eig_pairs])\n",
    "eig_vecs_sorted = np.array([x[1] for x in eig_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import KneeLocator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming eig_vals and eig_vals_sorted are already computed\n",
    "eig_vals_total = sum(eig_vals)\n",
    "explained_variance = [(i / eig_vals_total)*100 for i in eig_vals_sorted]\n",
    "explained_variance = np.round(explained_variance, 2)\n",
    "cum_explained_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Limit to first 200 components\n",
    "n_components = 200\n",
    "x = np.arange(1, n_components+1)\n",
    "explained_variance_truncated = explained_variance[:n_components]\n",
    "cum_explained_variance_truncated = cum_explained_variance[:n_components]\n",
    "\n",
    "# Find the elbow point using truncated data\n",
    "kneedle = KneeLocator(\n",
    "    x, \n",
    "    cum_explained_variance_truncated, \n",
    "    S=1,\n",
    "    curve=\"concave\", \n",
    "    direction=\"increasing\"\n",
    ")\n",
    "elbow_point = kneedle.knee\n",
    "\n",
    "# Find the point where cumulative variance stops increasing significantly\n",
    "# Here, we look for the first instance where the increment is less than a threshold\n",
    "increment_threshold = 0.001  # adjust threshold (in percentage points) as needed\n",
    "diffs = np.diff(cum_explained_variance_truncated)\n",
    "constant_indices = np.where(diffs < increment_threshold)[0]\n",
    "\n",
    "if constant_indices.size > 0:\n",
    "    constant_point = constant_indices[0] + 1  # shift index because of np.diff\n",
    "    constant_variance = cum_explained_variance_truncated[constant_point-1]\n",
    "else:\n",
    "    constant_point = None\n",
    "\n",
    "# Create figure with two y-axes\n",
    "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot cumulative variance on primary y-axis\n",
    "line1 = ax1.plot(x, cum_explained_variance_truncated, '-o', color='blue', \n",
    "                 label='Cumulative variance', markersize=3)\n",
    "ax1.set_xlabel('Number of components', fontsize=20)\n",
    "ax1.set_ylabel('Cumulative explained variance (%)', fontsize=20, color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue', labelsize=16)\n",
    "ax1.tick_params(axis='x', labelsize=16)\n",
    "\n",
    "# Plot histogram of individual variances on secondary y-axis\n",
    "bars = ax2.bar(x, explained_variance_truncated, alpha=0.3, color='gray', \n",
    "               label='Individual variance')\n",
    "ax2.set_ylabel('Individual explained variance (%)', fontsize=20, color='gray')\n",
    "ax2.tick_params(axis='y', labelcolor='gray', labelsize=16)\n",
    "\n",
    "# Add vertical line at elbow point\n",
    "if elbow_point:\n",
    "    ax1.axvline(x=elbow_point, color='r', linestyle='--', \n",
    "                label=f'Elbow point: PC{elbow_point}')\n",
    "    elbow_variance = cum_explained_variance_truncated[elbow_point-1]\n",
    "    ax1.axhline(y=elbow_variance, color='r', linestyle='--')\n",
    "    ax1.annotate(f'Variance: {elbow_variance:.1f}%', \n",
    "                 xy=(elbow_point, elbow_variance),\n",
    "                 xytext=(10, 10), textcoords='offset points',\n",
    "                 fontsize=12)\n",
    "\n",
    "# Add second marker when variance stops increasing\n",
    "if constant_point:\n",
    "    ax1.axvline(x=constant_point, color='green', linestyle='--', \n",
    "                label=f'Constant variance starts at PC{constant_point}')\n",
    "    ax1.axhline(y=constant_variance, color='green', linestyle='--')\n",
    "    ax1.annotate(f'Stationary variance: {constant_variance:.1f}%', \n",
    "                 xy=(constant_point, constant_variance),\n",
    "                 xytext=(10, -20), textcoords='offset points',\n",
    "                 fontsize=12)\n",
    "\n",
    "# Combine legends from both axes\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left', fontsize=12)\n",
    "\n",
    "# Remove grid and adjust layout\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if elbow_point:\n",
    "    print(f\"Elbow point occurs at PC{elbow_point} with {elbow_variance:.1f}% cumulative explained variance\")\n",
    "else:\n",
    "    print(\"No clear elbow point was found\")\n",
    "\n",
    "if constant_point:\n",
    "    print(f\"Cumulative variance becomes nearly constant at PC{constant_point} with {constant_variance:.1f}% variance\")\n",
    "else:\n",
    "    print(\"No point of constant variance was detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)  # for reproducibility\n",
    "\n",
    "# Select 6 random sample indices\n",
    "n_samples = data_matrix.shape[0]\n",
    "random_indices = np.random.choice(n_samples, 6, replace=False)\n",
    "\n",
    "# Calculate mean of the original data\n",
    "mean_data = np.average(data_matrix, weights=weights, axis=0)\n",
    "\n",
    "# Calculate PC scores for all samples\n",
    "pc_scores = (data_matrix - mean_data) @ eig_vecs_sorted.T\n",
    "\n",
    "# Define PC numbers to display\n",
    "pcs_row1 = [1, 2, 3, 4, 5]\n",
    "pcs_row2 = [10, 15, 25, 50, 100, 136]\n",
    "\n",
    "# Create figure with subplots for the 6 samples\n",
    "fig, axes = plt.subplots(12, 6, figsize=(20, 30))\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "for sample_num, sample_idx in enumerate(random_indices):\n",
    "   # Get row indices for this sample\n",
    "   row1_idx = sample_num * 2\n",
    "   row2_idx = sample_num * 2 + 1\n",
    "   \n",
    "   # Original image\n",
    "   original = data_matrix[sample_idx].reshape(98, 101)\n",
    "   \n",
    "   # Plot original in both rows\n",
    "   im0_1 = axes[row1_idx, 0].imshow(original, aspect='auto', cmap='viridis', origin='lower')\n",
    "   axes[row1_idx, 0].set_title(f'Original\\nSample {sample_idx}')\n",
    "   plt.colorbar(im0_1, ax=axes[row1_idx, 0])\n",
    "   \n",
    "   # First row reconstructions (1-5 PCs)\n",
    "   for i, n_pc in enumerate(pcs_row1):\n",
    "       reconstruction = (mean_data + pc_scores[sample_idx, :n_pc] @ eig_vecs_sorted[:n_pc]).reshape(98, 101)\n",
    "       im = axes[row1_idx, i+1].imshow(reconstruction, aspect='auto', cmap='viridis', origin='lower')\n",
    "       axes[row1_idx, i+1].set_title(f'{n_pc} PCs')\n",
    "       plt.colorbar(im, ax=axes[row1_idx, i+1])\n",
    "       \n",
    "   # Second row reconstructions (10-136 PCs)\n",
    "   for i, n_pc in enumerate(pcs_row2):\n",
    "       reconstruction = (mean_data + pc_scores[sample_idx, :n_pc] @ eig_vecs_sorted[:n_pc]).reshape(98, 101)\n",
    "       im = axes[row2_idx, i].imshow(reconstruction, aspect='auto', cmap='viridis', origin='lower')\n",
    "       axes[row2_idx, i].set_title(f'{n_pc} PCs')\n",
    "       plt.colorbar(im, ax=axes[row2_idx, i])\n",
    "\n",
    "# Print reconstruction errors\n",
    "for sample_idx in random_indices:\n",
    "   print(f\"\\nSample {sample_idx}:\")\n",
    "   \n",
    "   # Calculate errors for all PC numbers\n",
    "   all_pcs = pcs_row1 + pcs_row2\n",
    "   for n_pc in all_pcs:\n",
    "       reconstruction = mean_data + pc_scores[sample_idx, :n_pc] @ eig_vecs_sorted[:n_pc]\n",
    "       error = np.mean((data_matrix[sample_idx] - reconstruction)**2)\n",
    "       print(f\"{n_pc} PCs reconstruction MSE: {error:.6f}\")\n",
    "   \n",
    "   # Full reconstruction error\n",
    "   full_reconstruction = mean_data + pc_scores[sample_idx] @ eig_vecs_sorted\n",
    "   full_error = np.mean((data_matrix[sample_idx] - full_reconstruction)**2)\n",
    "   print(f\"Full reconstruction MSE: {full_error:.6f}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Calculate mean of the original data\n",
    "mean_data = np.average(data_matrix, weights=weights, axis=0)\n",
    "\n",
    "# Initialize arrays\n",
    "n_components_range = range(1, 151)\n",
    "mse_values = np.zeros(len(n_components_range))\n",
    "batch_size = 1000\n",
    "\n",
    "# Calculate number of batches\n",
    "n_samples = data_matrix.shape[0]\n",
    "n_batches = int(np.ceil(n_samples / batch_size))\n",
    "\n",
    "for i, n_comp in enumerate(tqdm(n_components_range)):\n",
    "    total_mse = 0\n",
    "    \n",
    "    for batch in range(n_batches):\n",
    "        start_idx = batch * batch_size\n",
    "        end_idx = min((batch + 1) * batch_size, n_samples)\n",
    "        \n",
    "        batch_data = data_matrix[start_idx:end_idx]\n",
    "        batch_scores = (batch_data - mean_data) @ eig_vecs_sorted[:n_comp].T\n",
    "        batch_reconstruction = mean_data + (batch_scores @ eig_vecs_sorted[:n_comp])\n",
    "        batch_mse = np.mean((batch_data - batch_reconstruction)**2, axis=1)\n",
    "        total_mse += np.sum(batch_mse)\n",
    "    \n",
    "    mse_values[i] = total_mse / n_samples\n",
    "\n",
    "# Calculate rate of change\n",
    "mse_diff = np.diff(mse_values)\n",
    "mse_rate = np.abs(mse_diff / mse_values[:-1]) * 100  # percentage change\n",
    "\n",
    "# Find plateau point (where rate of change becomes less than 1%)\n",
    "threshold = 1  # 1% change\n",
    "plateau_point = np.where(mse_rate < threshold)[0][0] + 1  # +1 because of diff\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Main plot with MSE values\n",
    "plt.plot(n_components_range, mse_values, '-o', markersize=3, label='MSE')\n",
    "\n",
    "# Add vertical line at plateau point\n",
    "plt.axvline(x=plateau_point, color='r', linestyle='--', \n",
    "            label=f'Plateau point: PC{plateau_point}')\n",
    "\n",
    "# Add annotation for plateau point\n",
    "plt.annotate(f'Plateau at PC{plateau_point}\\nMSE: {mse_values[plateau_point-1]:.6f}',\n",
    "            xy=(plateau_point, mse_values[plateau_point-1]),\n",
    "            xytext=(10, 10), textcoords='offset points',\n",
    "            bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5))\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Number of Principal Components', fontsize=12)\n",
    "plt.ylabel('Mean Squared Error (log scale)', fontsize=12)\n",
    "plt.title('Reconstruction Error vs Number of Principal Components\\nwith Plateau Detection', fontsize=14)\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print key values\n",
    "print(f\"\\nPlateau detected at PC{plateau_point}\")\n",
    "print(f\"MSE at plateau: {mse_values[plateau_point-1]:.6f}\")\n",
    "print(f\"Rate of change at plateau: {mse_rate[plateau_point-1]:.2f}%\")\n",
    "\n",
    "# Print first few rates of change for context\n",
    "print(\"\\nRate of change in first few components:\")\n",
    "for i in range(min(10, len(mse_rate))):\n",
    "    print(f\"PC{i+1} to PC{i+2}: {mse_rate[i]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "from scipy.signal import savgol_filter\n",
    "import os\n",
    "\n",
    "# --- Matplotlib settings ---\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['legend.fontsize'] = 12\n",
    "plt.rcParams['figure.titlesize'] = 18\n",
    "plt.rcParams['lines.linewidth'] = 2.0\n",
    "plt.rcParams['axes.linewidth'] = 1.5\n",
    "plt.rcParams['xtick.major.width'] = 1.5\n",
    "plt.rcParams['ytick.major.width'] = 1.5\n",
    "plt.rcParams['xtick.minor.width'] = 1.0\n",
    "plt.rcParams['ytick.minor.width'] = 1.0\n",
    "plt.rcParams['grid.linewidth'] = 0.8\n",
    "plt.rcParams['axes.labelpad'] = 10.0\n",
    "plt.rcParams['axes.titlepad'] = 12.0\n",
    "plt.rcParams['xtick.major.size'] = 6\n",
    "plt.rcParams['ytick.major.size'] = 6\n",
    "plt.rcParams['xtick.minor.size'] = 4\n",
    "plt.rcParams['ytick.minor.size'] = 4\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "plt.rcParams['legend.frameon'] = False\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['savefig.bbox'] = 'tight'\n",
    "plt.rcParams['savefig.transparent'] = False \n",
    "\n",
    "smoothed_mse = None\n",
    "first_derivative = None\n",
    "second_derivative = np.array([]) # Initialize as empty\n",
    "x_range = np.array([])\n",
    "transition_point = None\n",
    "curvature_threshold = 0.0\n",
    "\n",
    "try:\n",
    "    if len(mse_values) >= 11: # Condition for original savgol_filter to work\n",
    "        smoothed_mse = savgol_filter(mse_values, window_length=11, polyorder=3)\n",
    "        first_derivative = np.gradient(smoothed_mse)\n",
    "        second_derivative = np.gradient(first_derivative)\n",
    "    elif len(mse_values) >= 3 : # Minimal for any smoothing/gradient\n",
    "        print(f\"Warning: len(mse_values) is {len(mse_values)}, < 11. Savgol_filter with w=11,p=3 would fail.\")\n",
    "        print(\"Attempting gradient on raw data (deviation from original method if it errored).\")\n",
    "        # This is a fallback, original might have just errored.\n",
    "        first_derivative = np.gradient(mse_values)\n",
    "        second_derivative = np.gradient(first_derivative)\n",
    "    else:\n",
    "        print(\"Warning: mse_values too short for derivative calculations.\")\n",
    "\n",
    "    if len(second_derivative) > 0:\n",
    "        x_range = np.array(n_components_range)[:len(second_derivative)]\n",
    "        curvature_threshold = np.std(second_derivative) * 0.01\n",
    "        potential_indices = np.where(np.abs(second_derivative) < curvature_threshold)[0]\n",
    "        if len(potential_indices) > 0:\n",
    "            transition_point = potential_indices[0]\n",
    "        else:\n",
    "            print(\"Warning: No transition point found with current threshold. Original method would error.\")\n",
    "            print(\"Using fallback: minimum absolute second derivative.\")\n",
    "            if len(second_derivative) > 0:\n",
    "                transition_point = np.argmin(np.abs(second_derivative))\n",
    "            else: # Should not be reachable if len(second_derivative) > 0 check passed\n",
    "                transition_point = 0\n",
    "    elif len(mse_values) > 0 :\n",
    "        print(\"Warning: Second derivative array is effectively empty. Cannot determine transition point reliably.\")\n",
    "        transition_point = 0\n",
    "        if len(x_range) == 0 and len(n_components_range) > 0 : x_range = np.array(n_components_range) # Plot raw MSE if no derivatives\n",
    "    else: # mse_values is empty or too short\n",
    "        print(\"Error: mse_values is empty or too short. Cannot perform analysis.\")\n",
    "        # Fallback to prevent plotting errors\n",
    "        if len(mse_values) == 0: mse_values = np.array([0.1]); n_components_range = np.array([1])\n",
    "\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"Error during derivative calculation (likely Savgol or gradient): {e}\")\n",
    "    # transition_point remains None or its last assigned value\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during calculations: {e}\")\n",
    "\n",
    "\n",
    "# Plot\n",
    "fig, axs = plt.subplots(2, 1, figsize=(7, 7))\n",
    "\n",
    "# --- Main plot (MSE) ---\n",
    "ax = axs[0]\n",
    "ax.plot(n_components_range, mse_values, '-o', markersize=4, markerfacecolor='white', markeredgewidth=1.5, label='MSE')\n",
    "\n",
    "plot_transition_flag = False\n",
    "if transition_point is not None and \\\n",
    "   transition_point < len(n_components_range) and \\\n",
    "   transition_point < len(mse_values):\n",
    "    plot_transition_flag = True\n",
    "\n",
    "if plot_transition_flag:\n",
    "    tp_val_x_plot = n_components_range[transition_point]\n",
    "    tp_label_pc_reported = transition_point # Using index for PC label as per \"old method\"\n",
    "\n",
    "    ax.axvline(x=tp_val_x_plot, color='r', linestyle='--', linewidth=1.5,\n",
    "                label=f'Transition: PC{tp_label_pc_reported}')\n",
    "    ax.annotate(f'PC{tp_label_pc_reported}',\n",
    "                xy=(tp_val_x_plot, mse_values[transition_point]),\n",
    "                xytext=(tp_val_x_plot + 0.05 * (ax.get_xlim()[1] - ax.get_xlim()[0]),\n",
    "                        mse_values[transition_point] * (1.1 if ax.get_yscale() == 'log' else 1) + (0.05 * (ax.get_ylim()[1] - ax.get_ylim()[0]) if ax.get_yscale() != 'log' else 0) ),\n",
    "                bbox=dict(boxstyle='round,pad=0.3', fc='lightcoral', alpha=0.7, lw=0.5),\n",
    "                fontsize=plt.rcParams['legend.fontsize'] * 0.9)\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Number of Principal Components')\n",
    "ax.set_ylabel('MSE (log scale)')\n",
    "ax.set_title('MSE vs. Number of Components')\n",
    "ax.legend()\n",
    "ax.grid(True, which=\"both\", ls=\"-\", alpha=0.3, color='gray', linewidth=plt.rcParams['grid.linewidth'])\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "# --- Second derivative plot ---\n",
    "ax = axs[1]\n",
    "if len(x_range) > 0 and len(second_derivative) > 0 and len(x_range) == len(second_derivative):\n",
    "    ax.plot(x_range, second_derivative, label='Second Derivative', color='darkgreen')\n",
    "    if plot_transition_flag and transition_point < len(x_range) and transition_point < len(second_derivative):\n",
    "        tp_val_x_deriv_plot = x_range[transition_point]\n",
    "        tp_label_pc_deriv_reported = transition_point\n",
    "\n",
    "        ax.axvline(x=tp_val_x_deriv_plot, color='r', linestyle='--', linewidth=1.5,\n",
    "                    label=f'Transition: PC{tp_label_pc_deriv_reported}')\n",
    "        ax.axhline(y=curvature_threshold, color='orangered', linestyle=':', linewidth=1.2, label='Threshold')\n",
    "        ax.axhline(y=-curvature_threshold, color='orangered', linestyle=':', linewidth=1.2)\n",
    "        ax.annotate(f'PC{tp_label_pc_deriv_reported}',\n",
    "                    xy=(tp_val_x_deriv_plot, second_derivative[transition_point]),\n",
    "                    xytext=(tp_val_x_deriv_plot + 0.05 * (ax.get_xlim()[1] - ax.get_xlim()[0]),\n",
    "                            second_derivative[transition_point] + 0.1 * (ax.get_ylim()[1] - ax.get_ylim()[0])),\n",
    "                    bbox=dict(boxstyle='round,pad=0.3', fc='lightcoral', alpha=0.7, lw=0.5),\n",
    "                    fontsize=plt.rcParams['legend.fontsize'] * 0.9)\n",
    "    elif len(second_derivative) == 0 : # Explicitly handle if second_derivative became empty after checks\n",
    "        ax.text(0.5, 0.5, \"Second derivative data is empty.\", ha='center', va='center', transform=ax.transAxes)\n",
    "\n",
    "else:\n",
    "    ax.text(0.5, 0.5, \"Data for derivative plot is insufficient or mismatched.\", ha='center', va='center', transform=ax.transAxes)\n",
    "    if 'x_range' in locals() and 'second_derivative' in locals(): # Check if they exist before printing len\n",
    "      print(f\"Debug plot: len(x_range)={len(x_range)}, len(second_derivative)={len(second_derivative)}\")\n",
    "\n",
    "\n",
    "ax.set_xlabel('Number of Principal Components')\n",
    "ax.set_ylabel('Second Derivative')\n",
    "ax.set_title('Curvature Analysis (Second Derivative)')\n",
    "ax.legend()\n",
    "ax.grid(True, which=\"major\", ls=\"-\", alpha=0.3, color='gray', linewidth=plt.rcParams['grid.linewidth'])\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "\n",
    "plt.tight_layout(pad=1.5, h_pad=2.0)\n",
    "\n",
    "figures_dir = \"figures_publication_corrected\" # New directory\n",
    "os.makedirs(figures_dir, exist_ok=True)\n",
    "file_base = os.path.join(figures_dir, \"mse_curvature_analysis_pub_style_final_method\")\n",
    "plt.savefig(f\"{file_base}.png\")\n",
    "print(f\"Plot saved to: {file_base}.png\")\n",
    "plt.savefig(f\"{file_base}.pdf\")\n",
    "print(f\"Plot saved to: {file_base}.pdf\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if plot_transition_flag:\n",
    "    print(f\"\\nTransition point detected at PC{transition_point}\") # Uses index as PC\n",
    "    print(f\"MSE at transition: {mse_values[transition_point]:.6f}\")\n",
    "    if transition_point < len(second_derivative):\n",
    "        print(f\"Second derivative at transition: {second_derivative[transition_point]:.6f}\")\n",
    "    else:\n",
    "        print(f\"Second derivative at transition: N/A (index {transition_point} out of bounds for second_derivative len {len(second_derivative)})\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nCould not reliably report transition point values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals_sorted_37 = eig_vals_sorted[:37]\n",
    "eig_vecs_sorted_37 = eig_vecs_sorted[:37, :]\n",
    "principal_components = data_matrix @ eig_vecs_sorted_37.T\n",
    "PCs_df = pd.DataFrame(principal_components, columns=[f'PC{i+1}' for i in range(principal_components.shape[1])])\n",
    "PCs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([metadata_df, PCs_df], axis=1)\n",
    "df.rename(columns={'filename': 'file_name'}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nan = df.isna().sum()\n",
    "print(f\"Number of rows with NaNs: {num_nan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "PCs_scaled = StandardScaler().fit_transform(PCs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(random_state=42, n_neighbors=15, min_dist=1, n_components=2, metric='euclidean')\n",
    "X_umap = reducer.fit_transform(PCs_scaled)\n",
    "\n",
    "df['UMAP 1'] = X_umap[:, 0]\n",
    "df['UMAP 2'] = X_umap[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(random_state=42, n_neighbors=15, min_dist=1, n_components=3, metric='euclidean')\n",
    "X_umap = reducer.fit_transform(PCs_scaled)\n",
    "\n",
    "df['UMAP3D 1'] = X_umap[:, 0]\n",
    "df['UMAP3D 2'] = X_umap[:, 1]\n",
    "df['UMAP3D 3'] = X_umap[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('./data/traits_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import umap.umap_ as umap\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "df['family'] = df['family'].astype('category')\n",
    "\n",
    "# Create a custom color palette with distinct colors\n",
    "unique_families = df['family'].unique()\n",
    "\n",
    "def generate_many_distinct_colors(n):\n",
    "    \"\"\"\n",
    "    Generate distinct colors by combining multiple qualitative palettes\n",
    "    \"\"\"\n",
    "    palettes = [\n",
    "        sns.color_palette(\"Set1\", n_colors=9),\n",
    "        sns.color_palette(\"Set2\", n_colors=8),\n",
    "        sns.color_palette(\"Set3\", n_colors=12),\n",
    "        sns.color_palette(\"Paired\", n_colors=12),\n",
    "        sns.color_palette(\"Dark2\", n_colors=8),\n",
    "        sns.color_palette(\"Accent\", n_colors=8),\n",
    "        sns.color_palette(\"tab20\", n_colors=20),\n",
    "        sns.color_palette(\"tab20b\", n_colors=20),\n",
    "        sns.color_palette(\"tab20c\", n_colors=20),\n",
    "        sns.color_palette(\"husl\", n_colors=25)\n",
    "    ]\n",
    "    \n",
    "    all_colors = []\n",
    "    for palette in palettes:\n",
    "        hex_colors = [mcolors.rgb2hex(c) for c in palette]\n",
    "        all_colors.extend(hex_colors)\n",
    "    \n",
    "    all_colors = list(dict.fromkeys(all_colors))\n",
    "    \n",
    "    while len(all_colors) < n:\n",
    "        for color in all_colors[:]:\n",
    "            if len(all_colors) >= n:\n",
    "                break\n",
    "            rgb = mcolors.to_rgb(color)\n",
    "            modified_rgb = tuple(min(1.0, max(0.0, v + np.random.uniform(-0.2, 0.2))) for v in rgb)\n",
    "            all_colors.append(mcolors.rgb2hex(modified_rgb))\n",
    "    \n",
    "    np.random.shuffle(all_colors)\n",
    "    return all_colors[:n]\n",
    "\n",
    "# Generate colors and create color map\n",
    "custom_palette = generate_many_distinct_colors(len(unique_families))\n",
    "color_map = dict(zip(unique_families, custom_palette))\n",
    "\n",
    "def plot_scatter(df_plot, x_col, y_col, color_map, \n",
    "                 color_col='family', title=''):\n",
    "    \"\"\"\n",
    "    Plots df_plot[x_col] vs. df_plot[y_col] colored by 'family'.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for fam, color in color_map.items():\n",
    "        subset = df_plot[df_plot[color_col] == fam]\n",
    "        plt.scatter(\n",
    "            subset[x_col],\n",
    "            subset[y_col],\n",
    "            color=color,\n",
    "            alpha=1.0,  \n",
    "            s=15  \n",
    "        )\n",
    "    plt.xlabel(x_col)\n",
    "    plt.ylabel(y_col)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# List of PC columns\n",
    "pc_cols = [f\"PC{i}\" for i in range(1, 21)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: UMAP 1 vs. UMAP 2\n",
    "plot_scatter(\n",
    "    df,\n",
    "    'UMAP 1',\n",
    "    'UMAP 2',\n",
    "    color_map,\n",
    "    title=\"Original: UMAP 1 vs UMAP 2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def plot_umap_subplots_by_family_density_pub(df, x_col='UMAP 1', y_col='UMAP 2', output_filename_base=\"umap_family_density_unified_cb\"):\n",
    "    \"\"\"\n",
    "    Creates publication-quality UMAP subplots with a unified colorbar:\n",
    "      - Single colorbar at the bottom for all subplots\n",
    "      - Larger font sizes for publication\n",
    "      - Global density normalization across all families\n",
    "      - Outward-pointing ticks\n",
    "      - Light grey background points with colored density overlay\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Enhanced Publication Quality Settings ---\n",
    "    plt.rcParams['font.size'] = 14\n",
    "    plt.rcParams['axes.titlesize'] = 16\n",
    "    plt.rcParams['axes.labelsize'] = 16\n",
    "    plt.rcParams['xtick.labelsize'] = 13\n",
    "    plt.rcParams['ytick.labelsize'] = 13\n",
    "    plt.rcParams['legend.fontsize'] = 13\n",
    "    plt.rcParams['figure.titlesize'] = 18\n",
    "\n",
    "    plt.rcParams['lines.linewidth'] = 1.5\n",
    "    plt.rcParams['axes.linewidth'] = 1.2\n",
    "    plt.rcParams['xtick.major.width'] = 1.2\n",
    "    plt.rcParams['ytick.major.width'] = 1.2\n",
    "    plt.rcParams['xtick.major.size'] = 6\n",
    "    plt.rcParams['ytick.major.size'] = 6\n",
    "\n",
    "    plt.rcParams['axes.labelpad'] = 10.0\n",
    "    plt.rcParams['axes.titlepad'] = 12.0\n",
    "\n",
    "    plt.rcParams['xtick.direction'] = 'out'\n",
    "    plt.rcParams['ytick.direction'] = 'out'\n",
    "\n",
    "    plt.rcParams['savefig.dpi'] = 600\n",
    "    plt.rcParams['savefig.bbox'] = 'tight'\n",
    "    # --- End Settings ---\n",
    "\n",
    "    target_families = [\n",
    "        'Parulidae (New World Warblers)', 'Prunellidae (Accentors)', 'Icteridae (Troupials and Allies)',\n",
    "        'Nectariniidae (Sunbirds and Spiderhunters)', 'Conopophagidae (Gnateaters)', 'Grallariidae (Antpittas)',\n",
    "        'Passerellidae (New World Sparrows)', 'Urocynchramidae (Przevalski\\'s Pinktail)', 'Cotingidae (Cotingas)',\n",
    "        'Elachuridae (Spotted Elachura)', 'Rhinocryptidae (Tapaculos)', 'Melanocharitidae (Berrypeckers and Longbills)'\n",
    "    ]\n",
    "\n",
    "\n",
    "    df['family'] = df['family'].astype('category')\n",
    "\n",
    "    # --- Compute densities with individual normalization to 0-1 ---\n",
    "    family_data = {}\n",
    "    \n",
    "    for target_family in target_families:\n",
    "        subset = df[df['family'] == target_family]\n",
    "        x_coords = subset[x_col].values\n",
    "        y_coords = subset[y_col].values\n",
    "        \n",
    "        density_values = []\n",
    "        if len(x_coords) > 1:\n",
    "            try:\n",
    "                xy = np.vstack([x_coords, y_coords])\n",
    "                kde = gaussian_kde(xy)\n",
    "                density_values = kde(xy)\n",
    "                # Normalize to 0-1 for this family\n",
    "                d_min, d_max = density_values.min(), density_values.max()\n",
    "                if d_max > d_min:\n",
    "                    density_values = (density_values - d_min) / (d_max - d_min)\n",
    "                else:\n",
    "                    density_values = np.ones_like(density_values) * 0.5\n",
    "            except (np.linalg.LinAlgError, ValueError) as e:\n",
    "                print(f\"KDE failed for {target_family}, using uniform density.\")\n",
    "                density_values = np.ones_like(x_coords) * 0.5\n",
    "        elif len(x_coords) == 1:\n",
    "            density_values = np.array([0.5])\n",
    "        \n",
    "        family_data[target_family] = {\n",
    "            'x': x_coords,\n",
    "            'y': y_coords,\n",
    "            'density': density_values,\n",
    "            'n': len(subset)\n",
    "        }\n",
    "    \n",
    "    # Use 0-1 normalization for all plots\n",
    "    norm = Normalize(vmin=0, vmax=1)\n",
    "\n",
    "\n",
    "    n_rows, n_cols = 4, 3\n",
    "    fig = plt.figure(figsize=(n_cols * 4.5, n_rows * 4 + 0.8))\n",
    "    \n",
    "    # Create grid with space for colorbar at bottom\n",
    "    gs = fig.add_gridspec(n_rows + 1, n_cols, height_ratios=[1, 1, 1, 1, 0.1], \n",
    "                          hspace=0.5, wspace=0.25)\n",
    "    \n",
    "    axes = []\n",
    "    for row in range(n_rows):\n",
    "        for col in range(n_cols):\n",
    "            axes.append(fig.add_subplot(gs[row, col]))\n",
    "\n",
    "    # Plot each family\n",
    "    for i, target_family in enumerate(target_families):\n",
    "        ax = axes[i]\n",
    "\n",
    "        # Background: all points in light grey\n",
    "        ax.scatter(df[x_col], df[y_col],\n",
    "                   color='lightgrey', s=5, alpha=0.3, rasterized=True)\n",
    "\n",
    "        # Foreground: target family with density coloring\n",
    "        data = family_data[target_family]\n",
    "        if len(data['x']) > 0:\n",
    "            sc = ax.scatter(data['x'], data['y'], \n",
    "                          c=data['density'], \n",
    "                          cmap='viridis', \n",
    "                          s=20,\n",
    "                          linewidth=0.2, \n",
    "                          norm=norm,  # Shared normalization\n",
    "                          rasterized=True)\n",
    "\n",
    "        # Extract only Latin name (remove English name in parentheses)\n",
    "        latin_name = target_family.split(' (')[0]\n",
    "        ax.set_title(f\"{latin_name}\\n(n={data['n']})\", fontsize=14)\n",
    "\n",
    "        # Axis labels only on outer plots\n",
    "        if i // n_cols == n_rows - 1:\n",
    "            ax.set_xlabel(x_col)\n",
    "        if i % n_cols == 0:\n",
    "            ax.set_ylabel(y_col)\n",
    "\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.tick_params(axis='both', which='major', pad=5)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for j in range(len(target_families), len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "\n",
    "    cbar_ax = fig.add_subplot(gs[-1, 1]) \n",
    "    cbar = fig.colorbar(sc, cax=cbar_ax, orientation='horizontal')\n",
    "    cbar.set_label('Relative Density (0 = low, 1 = high within family)', fontsize=16, labelpad=10)\n",
    "    cbar.ax.tick_params(labelsize=13)\n",
    "\n",
    "\n",
    "    # --- Save figures ---\n",
    "    figures_dir = \"figures\"\n",
    "    try:\n",
    "        os.makedirs(figures_dir, exist_ok=True)\n",
    "        \n",
    "        file_path_png = os.path.join(figures_dir, f\"{output_filename_base}.png\")\n",
    "        plt.savefig(file_path_png)\n",
    "        print(f\"Plot saved to: {file_path_png}\")\n",
    "\n",
    "        file_path_pdf = os.path.join(figures_dir, f\"{output_filename_base}.pdf\")\n",
    "        plt.savefig(file_path_pdf)\n",
    "        print(f\"Plot saved to: {file_path_pdf}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving figures: {e}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Execute the function\n",
    "plot_umap_subplots_by_family_density_pub(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def plot_umap_subplots_by_family_species_density_pub(df, x_col='UMAP 1', y_col='UMAP 2', species_col='species',\n",
    "                                                  output_filename_base=\"umap_family_species_density\"):\n",
    "    \"\"\"\n",
    "    Creates publication-quality subplots for specific families, coloring density by up to 3 species per family.\n",
    "    Features a single common legend at the bottom with generic species labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Enhanced Publication Quality Settings ---\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans']\n",
    "    plt.rcParams['font.size'] = 14\n",
    "    plt.rcParams['axes.titlesize'] = 16\n",
    "    plt.rcParams['axes.labelsize'] = 16\n",
    "    plt.rcParams['xtick.labelsize'] = 13\n",
    "    plt.rcParams['ytick.labelsize'] = 13\n",
    "    plt.rcParams['legend.fontsize'] = 14\n",
    "    plt.rcParams['figure.titlesize'] = 18\n",
    "\n",
    "    plt.rcParams['lines.linewidth'] = 1.5\n",
    "    plt.rcParams['axes.linewidth'] = 1.2\n",
    "    plt.rcParams['xtick.major.width'] = 1.2\n",
    "    plt.rcParams['ytick.major.width'] = 1.2\n",
    "    plt.rcParams['xtick.major.size'] = 6\n",
    "    plt.rcParams['ytick.major.size'] = 6\n",
    "\n",
    "    plt.rcParams['axes.labelpad'] = 10.0\n",
    "    plt.rcParams['axes.titlepad'] = 12.0\n",
    "\n",
    "    plt.rcParams['xtick.direction'] = 'out'\n",
    "    plt.rcParams['ytick.direction'] = 'out'\n",
    "\n",
    "    plt.rcParams['savefig.dpi'] = 600\n",
    "    plt.rcParams['savefig.bbox'] = 'tight'\n",
    "    plt.rcParams['legend.frameon'] = False\n",
    "    # --- End Settings ---\n",
    "\n",
    "    # Define custom color maps\n",
    "    custom_blue = LinearSegmentedColormap.from_list('custom_blue', ['#e0eaff', '#6699ff', '#0033cc'])\n",
    "    custom_green = LinearSegmentedColormap.from_list('custom_green', ['#e6ffe6', '#66cc66', '#006600'])\n",
    "    custom_red = LinearSegmentedColormap.from_list('custom_red', ['#ffe6e6', '#ff6666', '#990000'])\n",
    "    \n",
    "    colormaps = [custom_blue, custom_green, custom_red]\n",
    "    max_species_to_plot = 3\n",
    "\n",
    "    target_families = [\n",
    "        'Parulidae (New World Warblers)', 'Prunellidae (Accentors)', 'Icteridae (Troupials and Allies)',\n",
    "        'Nectariniidae (Sunbirds and Spiderhunters)', 'Conopophagidae (Gnateaters)', 'Grallariidae (Antpittas)',\n",
    "        'Passerellidae (New World Sparrows)', 'Urocynchramidae (Przevalski\\'s Pinktail)', 'Cotingidae (Cotingas)',\n",
    "        'Elachuridae (Spotted Elachura)', 'Rhinocryptidae (Tapaculos)', 'Melanocharitidae (Berrypeckers and Longbills)'\n",
    "    ]\n",
    "\n",
    "    # --- Input Validation ---\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        print(\"Error: 'df' is not a pandas DataFrame.\")\n",
    "        return\n",
    "    for col in ['family', species_col, x_col, y_col]:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Error: Required column '{col}' not found in DataFrame.\")\n",
    "            return\n",
    "    # --- End Validation ---\n",
    "\n",
    "    df['family'] = df['family'].astype('category')\n",
    "\n",
    "    n_rows, n_cols = 4, 3\n",
    "    \n",
    "    # Create figure with space for legend at bottom\n",
    "    fig = plt.figure(figsize=(n_cols * 4.5, n_rows * 4 + 0.8))\n",
    "    gs = fig.add_gridspec(n_rows + 1, n_cols, height_ratios=[1, 1, 1, 1, 0.15], \n",
    "                          hspace=0.5, wspace=0.25)\n",
    "    \n",
    "    axes = []\n",
    "    for row in range(n_rows):\n",
    "        for col in range(n_cols):\n",
    "            axes.append(fig.add_subplot(gs[row, col]))\n",
    "\n",
    "    for i_ax, target_family in enumerate(target_families):\n",
    "        ax = axes[i_ax]\n",
    "\n",
    "        # Background: all points in light grey\n",
    "        ax.scatter(df[x_col], df[y_col],\n",
    "                   color='#e8e8e8', s=8, alpha=0.25, rasterized=True, zorder=1)\n",
    "\n",
    "        family_subset = df[df['family'] == target_family].copy()\n",
    "        n_samples_family = len(family_subset)\n",
    "\n",
    "        # Get top 3 species by count\n",
    "        if not family_subset.empty:\n",
    "            species_counts = family_subset[species_col].value_counts()\n",
    "            top_species_in_family = species_counts.nlargest(max_species_to_plot).index.tolist()\n",
    "        else:\n",
    "            top_species_in_family = []\n",
    "\n",
    "        for i_species, species_name in enumerate(top_species_in_family):\n",
    "            species_subset = family_subset[family_subset[species_col] == species_name]\n",
    "            x_species = species_subset[x_col].values\n",
    "            y_species = species_subset[y_col].values\n",
    "\n",
    "            if len(x_species) == 0:\n",
    "                continue\n",
    "\n",
    "            density_species_normalized = np.ones_like(x_species, dtype=float)\n",
    "            if len(x_species) > 1:\n",
    "                try:\n",
    "                    xy_species = np.vstack([x_species, y_species])\n",
    "                    kde_species = gaussian_kde(xy_species)\n",
    "                    density_species = kde_species(xy_species)\n",
    "                    \n",
    "                    min_d, max_d = density_species.min(), density_species.max()\n",
    "                    if max_d > min_d:\n",
    "                        density_species_normalized = (density_species - min_d) / (max_d - min_d)\n",
    "                    else:\n",
    "                        density_species_normalized = np.ones_like(density_species) * 0.7\n",
    "                except (np.linalg.LinAlgError, ValueError) as e:\n",
    "                    print(f\"KDE failed for {species_name} in {target_family}. Using uniform density.\")\n",
    "                    density_species_normalized = np.ones_like(x_species) * 0.7\n",
    "\n",
    "            cmap_species = colormaps[i_species % len(colormaps)]\n",
    "            ax.scatter(x_species, y_species,\n",
    "                       c=density_species_normalized,\n",
    "                       cmap=cmap_species,\n",
    "                       s=25,\n",
    "                       linewidth=0.1, alpha=0.85, rasterized=True, zorder=i_species + 2)\n",
    "\n",
    "        # Extract only Latin name (remove English name in parentheses)\n",
    "        latin_name = target_family.split(' (')[0]\n",
    "        ax.set_title(f\"{latin_name}\\n(n={n_samples_family})\", fontsize=14)\n",
    "\n",
    "        # Axis labels only on outer plots\n",
    "        if i_ax // n_cols == n_rows - 1:\n",
    "            ax.set_xlabel(x_col)\n",
    "        if i_ax % n_cols == 0:\n",
    "            ax.set_ylabel(y_col)\n",
    "\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.tick_params(axis='both', which='major', pad=5)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for j in range(len(target_families), len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    # --- Add common legend at bottom ---\n",
    "    legend_ax = fig.add_subplot(gs[-1, :])\n",
    "    legend_ax.axis('off')\n",
    "    \n",
    "    # Create generic legend handles\n",
    "    legend_handles = []\n",
    "    species_labels = ['Species 1', 'Species 2', 'Species 3']\n",
    "    for i in range(3):\n",
    "        legend_handles.append(plt.Line2D([0], [0], marker='o', color='w',\n",
    "                                        label=species_labels[i],\n",
    "                                        markerfacecolor=colormaps[i](0.75),\n",
    "                                        markersize=10, markeredgewidth=0))\n",
    "    \n",
    "    legend_ax.legend(handles=legend_handles, loc='center', ncol=3, \n",
    "                    fontsize=14, frameon=False,\n",
    "                    columnspacing=2.0, handletextpad=0.5)\n",
    "    # --- End legend ---\n",
    "\n",
    "    # --- Save figures ---\n",
    "    figures_dir = \"figures\"\n",
    "    try:\n",
    "        os.makedirs(figures_dir, exist_ok=True)\n",
    "        print(f\"Directory '{figures_dir}' ensured.\")\n",
    "        \n",
    "        file_path_png = os.path.join(figures_dir, f\"{output_filename_base}.png\")\n",
    "        plt.savefig(file_path_png)\n",
    "        print(f\"Plot saved to: {file_path_png}\")\n",
    "        \n",
    "        file_path_pdf = os.path.join(figures_dir, f\"{output_filename_base}.pdf\")\n",
    "        plt.savefig(file_path_pdf)\n",
    "        print(f\"Plot saved to: {file_path_pdf}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving figures: {e}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Execute the function\n",
    "plot_umap_subplots_by_family_species_density_pub(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}